{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9926ba16",
   "metadata": {},
   "source": [
    "# Computation of individual phenotypes from shap profiles\n",
    "\n",
    "The core function to generate shap values is `get_shaps()` below. Some notes on its usage:\n",
    "\n",
    "- abundance data should be a pandas dataframe consisting of abundance data in wide form, i.e. `dat` should have:\n",
    "  - first column: `subject_id` column\n",
    "  - remaining columns: relative abundance values matching the features (taxa/timepoint) of the cohort models\n",
    "  - can also provided pre-quantized data as `dat_quant`\n",
    "- It is recommended to run the shap analysis several times to improve the quality of estimates\n",
    "\n",
    "Two main approaches are supported - providing modeling data, and providing existing models:\n",
    "- Data can provided to retrain cohort models with each run of the shap analysis\n",
    "  - Data should be in long form (see examples in `qbiome` package) - pass as `appr_data` and `sub_data`; or in wide form, with subject abundances already quantized\n",
    "  - Quantizer can be provided, or omitted and generated from the data\n",
    "- Can also use pretrained models (`ahcg_list` and `shcg_list`), which are recycled as necessary to complete the requested number of runs (`n_runs`) \n",
    "  - In this case, a quantizer must also be provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86eb1c6-a536-4832-94aa-7c39ec395b1e",
   "metadata": {},
   "source": [
    "# Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f228e477-826b-4416-8a39-f31a0453ff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import qbiome.data_formatter\n",
    "import qbiome.forecaster\n",
    "import qbiome.hypothesis\n",
    "import qbiome.qnet_orchestrator\n",
    "import qbiome.quantizer\n",
    "import qbiome.qutil\n",
    "import shap\n",
    "from quasinet import qnet, qsampling\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "appr_data_quant = pd.read_csv(\"uchicago_appropriate_cohort_quantized.csv\")\n",
    "sub_data_quant = pd.read_csv(\"uchicago_suboptimal_cohort_quantized.csv\")\n",
    "\n",
    "boston_data_quant = pd.read_csv(\"boston_data_quantized.csv\")\n",
    "\n",
    "qnt = qbiome.quantizer.Quantizer(num_levels=26)\n",
    "qnt.load_quantizer_states(\"quantizer.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27d0f825",
   "metadata": {},
   "source": [
    "## Data format for new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96868f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>Acidimicrobiia_25</th>\n",
       "      <th>Acidimicrobiia_26</th>\n",
       "      <th>Acidimicrobiia_27</th>\n",
       "      <th>Acidimicrobiia_28</th>\n",
       "      <th>Acidimicrobiia_29</th>\n",
       "      <th>Acidimicrobiia_30</th>\n",
       "      <th>Acidimicrobiia_31</th>\n",
       "      <th>Acidimicrobiia_32</th>\n",
       "      <th>Acidimicrobiia_33</th>\n",
       "      <th>...</th>\n",
       "      <th>unclassified_Verrucomicrobiota_27</th>\n",
       "      <th>unclassified_Verrucomicrobiota_28</th>\n",
       "      <th>unclassified_Verrucomicrobiota_29</th>\n",
       "      <th>unclassified_Verrucomicrobiota_30</th>\n",
       "      <th>unclassified_Verrucomicrobiota_31</th>\n",
       "      <th>unclassified_Verrucomicrobiota_32</th>\n",
       "      <th>unclassified_Verrucomicrobiota_33</th>\n",
       "      <th>unclassified_Verrucomicrobiota_34</th>\n",
       "      <th>unclassified_Verrucomicrobiota_35</th>\n",
       "      <th>unclassified_Verrucomicrobiota_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 1093 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id  Acidimicrobiia_25 Acidimicrobiia_26 Acidimicrobiia_27   \n",
       "0    136000.0                NaN               NaN               NaN  \\\n",
       "\n",
       "  Acidimicrobiia_28 Acidimicrobiia_29 Acidimicrobiia_30 Acidimicrobiia_31   \n",
       "0               NaN               NaN               NaN                 M  \\\n",
       "\n",
       "  Acidimicrobiia_32 Acidimicrobiia_33  ... unclassified_Verrucomicrobiota_27   \n",
       "0                 M                 M  ...                               NaN  \\\n",
       "\n",
       "  unclassified_Verrucomicrobiota_28 unclassified_Verrucomicrobiota_29   \n",
       "0                               NaN                               NaN  \\\n",
       "\n",
       "   unclassified_Verrucomicrobiota_30 unclassified_Verrucomicrobiota_31   \n",
       "0                                NaN                                 M  \\\n",
       "\n",
       "  unclassified_Verrucomicrobiota_32 unclassified_Verrucomicrobiota_33   \n",
       "0                                 M                                 M  \\\n",
       "\n",
       "  unclassified_Verrucomicrobiota_34 unclassified_Verrucomicrobiota_35   \n",
       "0                                 M                                 M  \\\n",
       "\n",
       "  unclassified_Verrucomicrobiota_36  \n",
       "0                                 M  \n",
       "\n",
       "[1 rows x 1093 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data_quant.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64460d2c-92d0-4c61-bf8e-150456a8b988",
   "metadata": {},
   "source": [
    "# Compute SHAP values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e4e1330",
   "metadata": {},
   "source": [
    "## Load existing models, if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c86b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "appr_qnet_lst = [qnet.load_qnet(\"shap-ahcg-1.joblib\"), qnet.load_qnet(\"shap-ahcg-2.joblib\")]\n",
    "sub_qnet_lst = [qnet.load_qnet(\"shap-shcg-1.joblib\"), qnet.load_qnet(\"shap-shcg-2.joblib\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98eba7c4",
   "metadata": {},
   "source": [
    "## SHAP computation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda6a05e-a236-4585-81c4-41ec038730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shaps(\n",
    "    dat=None,\n",
    "    dat_quant=None,\n",
    "    appr_data=None,\n",
    "    sub_data=None,\n",
    "    appr_data_quant=None,\n",
    "    sub_data_quant=None,\n",
    "    nsamples=1100,\n",
    "    quantizer=None,\n",
    "    ahcg_lst=None,\n",
    "    shcg_lst=None,\n",
    "    n_qsamps=3,\n",
    "    n_runs=1,\n",
    "    shap_dir=\"tmp_shap/\",\n",
    "    save_qnets=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes shap values for data `dat`.\n",
    "\n",
    "            Parameters:\n",
    "                    dat (pandas.core.frame.DataFrame): dataframe of relative abundances (wide form)\n",
    "                    dat_quant (pandas.core.frame.DataFrame): dataframe of quantized relative abundances (wide form)\n",
    "                    appr_data (pandas.core.frame.DataFrame): dataframe of abundance observations used to construct the appropriate cohort model\n",
    "                    sub_data (pandas.core.frame.DataFrame): dataframe of abundance observations used to construct the suboptimal cohort model\n",
    "                    appr_data_quant (pandas.core.frame.DataFrame): dataframe of quantized abundance observations used to construct the appropriate cohort model\n",
    "                    sub_data_quant (pandas.core.frame.DataFrame): dataframe of quantized abundance observations used to construct the suboptimal cohort model\n",
    "                    nsamples (int): passed to shap.KernelExplainer.shap_values()\n",
    "                    ahctg_lst (list): optional list of pretrained qnet models used for the appropriate cohort (one per run, recycled if needed). can be omitted and computed from provided data\n",
    "                    shctg_lst (list): optional list of pretrained qnet models used for the suboptimal cohort (one per run, recycled if needed). can be omitted and computed from provided data\n",
    "                    n_qsamps (int): number of qsamples to generate from each cohort to use as background data for the Shap computation\n",
    "                    n_runs (int): number of times to regenerate shap values for each subject in `dat`\n",
    "                    shap_dir (str): directory to save computed shap values and computed models if `save_qnets` is enabled\n",
    "                    save_qnets (bool): indicator of whether to save the qnet computed on each run\n",
    "\n",
    "            Returns:\n",
    "                    shaps (numpy.ndarray): shap values for final run\n",
    "                    explainer (shap.KernelExplainer): shap explainer for final run\n",
    "                    ahctg (Quasinet.Qnet): appropriate qnet model for final run\n",
    "                    shctg (Quasinet.Qnet): suboptimal qnet model for final run\n",
    "                    time (str): time of completion of final run\n",
    "                    \n",
    "    \"\"\"\n",
    "    if not os.path.isdir(shap_dir):\n",
    "        os.makedirs(shap_dir)\n",
    "\n",
    "    def _get_labels(num_lvls):\n",
    "        \"\"\"\n",
    "        Helper function in case we want to quantize into more than 26 levels\n",
    "        \"\"\"\n",
    "        import string\n",
    "        lbls = list(string.ascii_uppercase)\n",
    "        for i in range(2, int(np.ceil(num_lvls / 26)) + 1):\n",
    "            lbls = lbls + [char * i for char in string.ascii_uppercase]\n",
    "\n",
    "        lbls = tuple(lbls[:num_lvls])\n",
    "\n",
    "        return {lbl: idx for idx, lbl in enumerate(lbls)}\n",
    "\n",
    "    def _get_quantizer(data, num_levels=26):\n",
    "        \"\"\"\n",
    "        Construct a quantizer from provided `data`\n",
    "        \"\"\"\n",
    "        quantizer = qbiome.quantizer.Quantizer(num_levels=num_levels)\n",
    "        if num_levels > 26:\n",
    "            quantizer.labels = _get_labels(num_levels)\n",
    "        data_quantized = quantizer.quantize_df(data)\n",
    "\n",
    "        return quantizer\n",
    "\n",
    "    def _quant_df_to_np(df):\n",
    "        \"\"\"\n",
    "        Converted quantized pandas dataframe into numpy format for qnet computations\n",
    "        \"\"\"\n",
    "        df_np = np.char.replace(\n",
    "            df.drop(\"subject_id\", axis=1, errors=\"ignore\").to_numpy(dtype=\"str\"),\n",
    "            \"nan\",\n",
    "            \"\",\n",
    "        )\n",
    "        return df_np\n",
    "\n",
    "    def risk_np(x, ahcg, shcg):\n",
    "        \"\"\"\n",
    "        Compute risk from quantized numpy sequence x using appropriate and suboptimal qnets\n",
    "        \"\"\"\n",
    "        theta_s = qnet.qdistance(np.full_like(x, fill_value=\"\"), x, shcg, shcg)\n",
    "        theta_a = qnet.qdistance(np.full_like(x, fill_value=\"\"), x, ahcg, ahcg)\n",
    "        if theta_a > 0:\n",
    "            risk = theta_s / theta_a\n",
    "        elif theta_s > 0:\n",
    "            risk = 100\n",
    "        else:\n",
    "            # risk = np.nan\n",
    "            risk = 1\n",
    "\n",
    "        return risk\n",
    "    \n",
    "    if dat is None and dat_quant is None:\n",
    "        raise Exception(\"Data for new subjects not provided. Pass either dat or dat_quant.\")\n",
    "\n",
    "    if quantizer is None:\n",
    "        if appr_data is not None and sub_data is not None:\n",
    "            quantizer = _get_quantizer(pd.concat([appr_data, sub_data]))\n",
    "        else:\n",
    "            raise Exception(\"Quantizer must be provided if appr_data and sub_data are not provided.\")\n",
    "\n",
    "    def __get_qnet(\n",
    "        data=None, data_quantized=None, num_levels=26, quantizer=quantizer, alpha=0.3, min_samples_split=2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute qnet from provided data\n",
    "        \"\"\"\n",
    "        if data is None and data_quantized is None:\n",
    "            raise Exception(\"Either data or data_quantized must be provided.\")\n",
    "        if quantizer is None:\n",
    "            quantizer = qbiome.quantizer.Quantizer(num_levels=num_levels)\n",
    "        if num_levels > 26:\n",
    "            quantizer.labels = _get_labels(num_levels)\n",
    "        orchestrator = qbiome.qnet_orchestrator.QnetOrchestrator(quantizer)\n",
    "        if data_quantized is None:\n",
    "            data_quantized = quantizer.quantize_df(data)\n",
    "        features, label_matrix = quantizer.get_qnet_inputs(data_quantized)\n",
    "        orchestrator.train_qnet(\n",
    "            features,\n",
    "            label_matrix,\n",
    "            alpha=alpha,\n",
    "            min_samples_split=min_samples_split,\n",
    "        )\n",
    "\n",
    "        return orchestrator.model\n",
    "    \n",
    "    if ahcg_lst is not None and shcg_lst is not None:\n",
    "        ahcg_lst = (ahcg_lst * n_runs)[:n_runs]\n",
    "        shcg_lst = (shcg_lst * n_runs)[:n_runs]\n",
    "    elif appr_data is None and appr_data_quant is None:\n",
    "        raise Exception(\"Either appr_data/sub_data, appr_data_quant/sub_data_quant, or ahcg_lst/shcg_lst are required\")\n",
    "\n",
    "    if verbose is True:\n",
    "        print(\"Starting SHAP runs\\n\")\n",
    "    for i in tqdm(range(n_runs), desc=\"n_runs\"):\n",
    "        if ahcg_lst is None:\n",
    "            if verbose is True:\n",
    "                print(\"Fitting approriate cohort model\\n\")\n",
    "            if appr_data is not None:\n",
    "                ahcg = __get_qnet(data=appr_data)\n",
    "            else:\n",
    "                ahcg = __get_qnet(data_quantized=appr_data_quant)\n",
    "        else:\n",
    "            ahcg = ahcg_lst[i]\n",
    "        if shcg_lst is None:\n",
    "            if verbose is True:\n",
    "                print(\"Fitting suboptimal cohort model\\n\")\n",
    "            if sub_data is not None:\n",
    "                shcg = __get_qnet(data=sub_data)\n",
    "            else:\n",
    "                shcg = __get_qnet(data_quantized=sub_data_quant)\n",
    "        else:\n",
    "            shcg = shcg_lst[i]\n",
    "\n",
    "        def __risk_fquant_shap(x):\n",
    "            r = np.array([risk_np(s, ahcg, shcg) for s in x])\n",
    "\n",
    "            return r\n",
    "\n",
    "        shaps = []\n",
    "\n",
    "        background_data = np.vstack(\n",
    "            (\n",
    "                np.array(\n",
    "                    [\n",
    "                        qsampling.qsample(\n",
    "                            np.full(len(ahcg.feature_names), fill_value=\"\"),\n",
    "                            ahcg,\n",
    "                            5000,\n",
    "                        )\n",
    "                        for i in range(n_qsamps)\n",
    "                    ]\n",
    "                ),\n",
    "                np.array(\n",
    "                    [\n",
    "                        qsampling.qsample(\n",
    "                            np.full(len(shcg.feature_names), fill_value=\"\"),\n",
    "                            shcg,\n",
    "                            5000,\n",
    "                        )\n",
    "                        for i in range(n_qsamps)\n",
    "                    ]\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        explainer = shap.KernelExplainer(\n",
    "            __risk_fquant_shap,\n",
    "            background_data,\n",
    "        )\n",
    "\n",
    "        if dat_quant is None:\n",
    "            dat_q_np = _quant_df_to_np(quantizer._quantize_df(dat))\n",
    "        else:\n",
    "            dat_q_np = _quant_df_to_np(dat_quant)\n",
    "\n",
    "        if verbose is True:\n",
    "            print(\"Computing SHAP values for subjects this run:\\n\")\n",
    "        for s in tqdm(dat_q_np, desc=\"subjects\"):\n",
    "            shaps.append(explainer.shap_values(s, nsamples=nsamples))\n",
    "\n",
    "        time = str(datetime.now())\n",
    "\n",
    "        np.savetxt(\n",
    "            shap_dir + \"shap_vals_risk \" + time + \".csv\",\n",
    "            np.array(shaps),\n",
    "            delimiter=\",\",\n",
    "        )\n",
    "\n",
    "        if save_qnets is True:\n",
    "            qnet.save_qnet(\n",
    "                ahcg, shap_dir + \"qnet-appr-\" + time + \".joblib\"\n",
    "            )\n",
    "            os.remove(shap_dir + \"qnet-appr-\" + time + \".joblib\")\n",
    "            qnet.save_qnet(\n",
    "                shcg, shap_dir + \"qnet-sub-\" + time + \".joblib\"\n",
    "            )\n",
    "            os.remove(shap_dir + \"qnet-sub-\" + time + \".joblib\")\n",
    "\n",
    "    return np.array(shaps), explainer, ahcg, shcg, time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebcbc52f",
   "metadata": {},
   "source": [
    "## Compute Shap values from existing models, quantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730d8573-8197-4ce7-8fab-4b65e336463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SHAP runs\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ae8b8c9eb84a84a2cc3652524a44b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_runs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP values for subjects this run:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "556dd0c8ab14462cbc8c0091b618a2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subjects:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SHAP values for subjects this run:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9299d4e374d4accabd95b82b3a96046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subjects:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_shaps(\n",
    "    dat_quant=boston_data_quant.head(1),\n",
    "    #appr_data=appr_data,\n",
    "    #sub_data=sub_data,\n",
    "    quantizer=qnt,\n",
    "    ahcg_lst=appr_qnet_lst,\n",
    "    shcg_lst=sub_qnet_lst,\n",
    "    nsamples=1100,\n",
    "    n_qsamps=2,\n",
    "    n_runs=2,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb867852-46f4-4ff1-9281-9b967cfc8835",
   "metadata": {},
   "source": [
    "# Classify subject phenotypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5e9ba15",
   "metadata": {},
   "source": [
    "## Classification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b75df43-d031-4e30-bdac-6c1a7e6e2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_subjects(\n",
    "    subject_data=None,\n",
    "    subject_data_quant=None,\n",
    "    shap_df=None,\n",
    "    bacteroidia_regex=\"^(Bacteroidia_2)\",\n",
    "    actino_regex=\"^(Actinobacteria_2)\",\n",
    "    shap_dir=\"tmp_shap/\",\n",
    "    thresh=0.005,\n",
    "    appr_data=None,\n",
    "    sub_data=None,\n",
    "    appr_data_quant=None,\n",
    "    sub_data_quant=None,\n",
    "    nsamples=1100,\n",
    "    quantizer=None,\n",
    "    ahcg_lst=None,\n",
    "    shcg_lst=None,\n",
    "    n_qsamps=3,\n",
    "    n_runs=1,\n",
    "    save_qnets=False,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Classify subjects into interventional phenotypes.\n",
    "\n",
    "            Parameters:\n",
    "                    subject_data (pandas.core.frame.DataFrame): wide form abundance data of subjects for which to compute shap_values. incompatible with shap_df\n",
    "                    subject_data_quant (pandas.core.frame.DataFrame): wide form quantized abundance data of subjects for which to compute shap_values. incompatible with shap_df\n",
    "                    shap_df (pandas.core.frame.DataFrame): dataframe of shap values for subjects to be classified. incompatible with subject_data\n",
    "                    bacteroidia_regex (str): regex identifying relevant Bacteroidia variables on which to base interventional grouping\n",
    "                    actino_regex (str): regex identifying relevant Actinobacteria variables on which to base interventional grouping\n",
    "                    thresh (float): require all relevant shap values of a subject < `thresh` to assign subject to the corresponding phenotype\n",
    "                    appr_data (pandas.core.frame.DataFrame): (used with subject_data only) dataframe of abundance observations used to construct the appropriate cohort model\n",
    "                    sub_data (pandas.core.frame.DataFrame): (used with subject_data only) dataframe of abundance observations used to construct the suboptimal cohort model\n",
    "                    nsamples (int): (used with subject_data only) passed to shap.KernelExplainer.shap_values()\n",
    "                    ahctg_lst (list): (used with subject_data only) optional list of pretrained qnet models used for the appropriate cohort (one per run, recycled if needed). can be omitted and computed from provided data\n",
    "                    shctg_lst (list): (used with subject_data only) optional list of pretrained qnet models used for the suboptimal cohort (one per run, recycled if needed). can be omitted and computed from provided data\n",
    "                    n_qsamps (int): (used with subject_data only) number of qsamples to generate from each cohort to use as background data for the Shap computation\n",
    "                    n_runs (int): (used with subject_data only) number of times to regenerate shap values for each subject in `dat`\n",
    "                    shap_dir (str): (used with subject_data only) directory to save computed shap values and computed models if `save_qnets` is enabled\n",
    "                    save_qnets (bool): (used with subject_data only) indicator of whether to save the qnet computed on each run\n",
    "                    verbose (bool): (used with subject_data only) print status updates during shap computation\n",
    "\n",
    "            Returns:\n",
    "                    phenotype_df (pandas.core.frame.DataFrame): assignment of subjects by index into phenotype groupings\n",
    "    \"\"\"\n",
    "    \n",
    "    if subject_data is None and subject_data_quant is None:\n",
    "        if shap_df is None:\n",
    "            raise Exception(\"Either subject data or a shap dataframe must be provided.\")\n",
    "    if subject_data is not None or subject_data_quant is not None:\n",
    "        if shap_df is not None:\n",
    "            raise Exception(\"Provide either subject data or a shap dataframe, not both.\")\n",
    "    \n",
    "        \n",
    "    if shap_df is None:        \n",
    "        get_shaps(\n",
    "            dat=subject_data,\n",
    "            dat_quant=subject_data_quant,\n",
    "            appr_data=appr_data,\n",
    "            sub_data=sub_data,\n",
    "            appr_data_quant=appr_data_quant,\n",
    "            sub_data_quant=sub_data_quant,\n",
    "            quantizer=quantizer,\n",
    "            ahcg_lst=ahcg_lst,\n",
    "            shcg_lst=shcg_lst,\n",
    "            nsamples=nsamples,\n",
    "            n_qsamps=n_qsamps,\n",
    "            n_runs=n_runs,\n",
    "            save_qnets=save_qnets,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        import glob\n",
    "        shaps_mean = np.mean(\n",
    "        np.stack(\n",
    "            [np.loadtxt(file, delimiter=\",\") for file in glob.glob(shap_dir + \"*.csv\")],\n",
    "            axis=-1,\n",
    "        ),\n",
    "        axis=-1,\n",
    "        )\n",
    "\n",
    "        if subject_data is not None:\n",
    "            shap_df = pd.DataFrame(\n",
    "                data=[shaps_mean],\n",
    "                columns=subject_data.drop(\"subject_id\", axis=1).columns,\n",
    "            )\n",
    "        else:\n",
    "            shap_df = pd.DataFrame(\n",
    "                data=[shaps_mean],\n",
    "                columns=subject_data_quant.drop(\"subject_id\", axis=1).columns,\n",
    "            )\n",
    "    \n",
    "\n",
    "    bact_subs = (\n",
    "        shap_df.filter(regex=bacteroidia_regex, axis=1)\n",
    "        .max(axis=1)[lambda x: x < thresh]\n",
    "        .index.values\n",
    "    )\n",
    "    act_subs = (\n",
    "        shap_df.filter(regex=actino_regex, axis=1)\n",
    "        .max(axis=1)[lambda x: x < thresh]\n",
    "        .index.values\n",
    "    )\n",
    "    act_subs = [x for x in act_subs if x not in bact_subs]\n",
    "    confl_subs = [\n",
    "        x for x in shap_df.index.values if (x not in bact_subs) & (x not in act_subs)\n",
    "    ]\n",
    "\n",
    "    bact_df = (\n",
    "        pd.DataFrame(bact_subs)\n",
    "        .rename(columns={0: \"subject idx\"})\n",
    "        .assign(phenotype=\"Bacteroidia intervention\")\n",
    "    )\n",
    "    act_df = (\n",
    "        pd.DataFrame(act_subs)\n",
    "        .rename(columns={0: \"subject idx\"})\n",
    "        .assign(phenotype=\"Actinobacteria intervention\")\n",
    "    )\n",
    "    confl_df = (\n",
    "        pd.DataFrame(confl_subs)\n",
    "        .rename(columns={0: \"subject idx\"})\n",
    "        .assign(phenotype=\"Null intervention\")\n",
    "    )\n",
    "\n",
    "    return pd.concat([bact_df, act_df, confl_df])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74c2d920",
   "metadata": {},
   "source": [
    "## Classify from abundance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cebbef68-ceee-4e84-98e9-5738c2e4bd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SHAP runs\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4242ac914e4d42e0a202387e11447c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_runs:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting approriate cohort model\n",
      "\n",
      "Fitting suboptimal cohort model\n",
      "\n",
      "Computing SHAP values for subjects this run:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652f8f25c897451daa371d497246da05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subjects:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting approriate cohort model\n",
      "\n",
      "Fitting suboptimal cohort model\n",
      "\n",
      "Computing SHAP values for subjects this run:\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722c0fc563af4eef8e98e8d2ea04cd3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "subjects:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject idx</th>\n",
       "      <th>phenotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bacteroidia intervention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject idx                 phenotype\n",
       "0          0.0  Bacteroidia intervention"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_subjects(\n",
    "    subject_data_quant=boston_data_quant.head(1),\n",
    "    appr_data_quant=appr_data_quant,\n",
    "    sub_data_quant=sub_data_quant,\n",
    "    quantizer=qnt,\n",
    "    #ahcg_lst=a_lst,\n",
    "    #shcg_lst=s_lst,\n",
    "    nsamples=1100,\n",
    "    n_qsamps=2,\n",
    "    n_runs=2,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "678475e7",
   "metadata": {},
   "source": [
    "## Classify from precomputed shaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333c7df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject idx</th>\n",
       "      <th>phenotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Bacteroidia intervention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject idx                 phenotype\n",
       "0          0.0  Bacteroidia intervention"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "shap_dir = \"./tmp_shap/\"\n",
    "\n",
    "shaps_mean = np.mean(\n",
    "    np.stack(\n",
    "        [np.loadtxt(file, delimiter=\",\") for file in glob.glob(shap_dir + \"*.csv\")],\n",
    "        axis=-1,\n",
    "    ),\n",
    "    axis=-1,\n",
    ")\n",
    "\n",
    "shap_df = pd.DataFrame(\n",
    "    data=[shaps_mean],\n",
    "    columns=boston_data_quant.drop(\"subject_id\", axis=1).columns,\n",
    ")\n",
    "\n",
    "classify_subjects(shap_df=shap_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474fed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qbiome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
