<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>qbiome.hypothesis API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#666666;background-color:black;//mix-blend-mode:difference;color:#bbbbbb;z-index:3}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#66bb66;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#66FF66}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:transparent;padding:1px 4px;color:#FFA500;overflow-wrap:break-word}h1 code{background:transparent}pre{overflow-wrap:break-word;background:#111111;word-wrap:break-word;border:0;border-top:1px solid #666;border-bottom:1px solid #666;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#333333;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#aaeeaa;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;color:#bbbbbb;background-color:black;overflow-wrap:break-word}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qbiome.hypothesis</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pygraphviz as pgv
import networkx as nx
import numpy as np
import re
import sys
import os
from scipy import stats
import warnings
import pandas as pd
import glob
from tqdm import tqdm
warnings.filterwarnings(&#39;ignore&#39;)

class Hypothesis(object):
    &#34;&#34;&#34;Generate and analyze hypotheses from models inferred.  Assume biomes_timestamp format is biome_timestamp.
       Also assume that dotname or decision tree name format is biome_timestamp.dot

       Mathematical model of causal hypothesis:

       ---

       ```
       Local Marginal Regulation Coefficient: Aiming to estimate the up-regulatory/down-regulatory influence of a source organism/entity on a target organism/entity, where regulation effects are causally localized in time (future cannot affect the past) with limited memory, and potential confounding effects from other entities/organisms are marginalized out.
       ```

       Let us assume a general dependency between stochastic processes \(\\nu,u, \omega \) :

       $$ \\nu_t = \\phi(u_{\leftarrow t},\omega_{\leftarrow t}) $$

      We estimate the sign of \( \\alpha_t\) in a locally linear marginalized relationship \( \\nu_t = \\alpha_t u_{t&#39;} + c \) with \(t&#39; \in [ t-\delta, t] \) as follows:

    Attributes:
       qnet_orchestrator (qbiome.QnetOrchestrator): instance of qbiome.QnetOrchestrator with trained qnet model
       model_path (str, optional): ath to directory containing generated decision trees in dot format (Default value = None)
       no_self_loops (bool, optional): If True do not report self-loops in hypotheses  (Default value = True)
       causal_constraint (float, optional): lag of source inputs from target effects. &gt;= 0 is causal  (Default value = 0)
       total_samples (int, optional): total number of samples used to construct decision model  (Default value = 100)
       detailed_labels (bool, optional): if True, decision tree models have detailed output  (Default value = False)
       MAPNAME (str): path to dequantization map

    &#34;&#34;&#34;

####    def __init__(self,qnet_orchestrator,
#    model_path=None,
#    no_self_loops=True,
#    causal_constraint=0,
#    total_samples=100,
#    detailed_labels=False):

    def __init__(self,
                 qnet_orchestrator=None,
                 quantizer=None,
                 quantizer_mapfile=None,
                 model_path=None,
                 no_self_loops=True,
                 causal_constraint=0,
                 total_samples=100,
                 detailed_labels=False):
        &#34;&#34;&#34;

        &#34;&#34;&#34;

        self.time_start = None
        self.time_end = None

        self.model_path = model_path
        self.qnet_orchestrator = qnet_orchestrator
        self.quantizer = quantizer


        if all(v is None for v in[qnet_orchestrator,quantizer,quantizer_mapfile]):
            raise Exception(&#39;Either qnet_orchestrator or quantizer or quantizer_mapfile must be provided to Hypothesis&#39;)

        if self.qnet_orchestrator is not None:
            self.quantizer = self.qnet_orchestrator.quantizer
        if self.quantizer is not None:
            self.variable_bin_map = self.quantizer.variable_bin_map
        else:
            self.variable_bin_map = np.load(quantizer_mapfile, allow_pickle=True)

        self.biomes_timestamp = [x for x in self.variable_bin_map.keys()]

        self.biomes = list(set([&#39;_&#39;.join(x.split(&#39;_&#39;)[:-1])
                                for x in self.biomes_timestamp]))

        self.NMAP = self.variable_bin_map

        if self.quantizer is not None:
            self.LABELS = quantizer.labels
        else:
            warnings.warn(&#34;Using manually coded labels. Provide quantizer to Hypothesis&#34;)
            self.LABELS = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}

        self.total_samples = total_samples
        self.detailed_labels = detailed_labels

        self.decision_tree = None
        self.tree_labels = None
        self.tree_edgelabels = None
        self.TGT = None
        self.SRC = None
        self.no_self_loops = no_self_loops

        self.causal_constraint = causal_constraint
        self.hypotheses=pd.DataFrame(columns=[&#39;src&#39;,&#39;tgt&#39;,&#39;time_tgt&#39;,&#39;lomar&#39;,&#39;pvalue&#39;])


    def src_time_constraint(self,source,target):
        &#34;&#34;&#34;Constrain which time points for source can be
           considered as inputs when computing source to target influences.
           If causal_constraint is None, there are no restrictions.
           Otherwise, only sources that are at least causal_constraint
           units of time prior to the target may be considered. Default is 0.
           Negative values are possible.

        Args:
          source (str): full source name in biome_timestamp format
          target (str): full target name in biome_timestamp format

        Returns:
          bool: Truth value of src acceptance

        &#34;&#34;&#34;
        _, source_biome_full, source_timestamp, _ \
            = re.split(r&#39;(.*)_(\d+)&#39;, source)
        _, target_biome_full, target_timestamp, _ \
            = re.split(r&#39;(.*)_(\d+)&#39;, target)

        if self.causal_constraint is not None:
            if (float(source_timestamp)
            + self.causal_constraint
                &lt;= float(target_timestamp)):
                return True
            return False
        return True


    def deQuantize_lowlevel(self,
                    letter,
                    bin_arr):
        &#34;&#34;&#34;Low level dequantizer function

        Args:
          letter (str): quantized level (str) or nan
          bin_arr (numpy.ndarray): 1D array of floats, which are quantization levels from trained quantizer.

        Returns:
          float: dequantized value

        &#34;&#34;&#34;

        if letter is np.nan or letter == &#39;nan&#39;:
            return np.nan
        lo = self.LABELS[letter]
        hi = lo + 1
        val = (bin_arr[lo] + bin_arr[hi]) / 2
        return val


    def deQuantizer(self,
                    letter,
                    biome_prefix,
                    timestamp_list=None,
                    time_start=None,
                    time_end=None):
        &#34;&#34;&#34;Dequantizer function calling low level deQuantize_lowlevel to account
           for the possibility of multiple timestamps being averaged over,
           and ability to operate with incomplete biome names.

        Args:
          letter (str): quantized level (str) or nan
          biome_prefix (str): prefix of biome name
          timestamp_list (numpy.ndarray, optional): 1D array of int time-stamps to consider. (Default value = None)
          time_start (int, optional): Start time. (Default value = None)
          time_end (int, optional): End time. (Default value = None)

        Returns:
          float: median of dequantized value

        &#34;&#34;&#34;
        vals = []
        if timestamp_list is None:
            if time_start is None and self.time_end is None:
                # average over all
                for biome_key in self.NMAP:
                    if biome_prefix in biome_key:
                        vals.append(
                            self.deQuantize_lowlevel(letter,
                                             self.NMAP[biome_key]))
            elif time_start is None:
                time_end = int(time_end)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ \
                        = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time &lt;= time_end) and (biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
            elif time_end is None:
                time_start = int(time_start)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time_start &lt;= time) and (biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
            else: # both present
                time_start = int(time_start)
                time_end = int(time_end)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time_start &lt;= time &lt;= time_end) and (
                            biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter,
                                             self.NMAP[biome_key]))
        else:
            for biome_key in self.NMAP:
                _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                time = int(time)
                if time in timestamp_list:
                    vals.append(self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))

        return np.median(vals)


    def getNumeric_internal(self,
               dict_id_reached_by_edgelabel,
               bin_name,
               timestamp_list=None,
               t0=None,
               t1=None):
        &#34;&#34;&#34;Dequantize labels on graph non-leaf nodes

        Args:
          dict_id_reached_by_edgelabel (dict[int,list[str]]): dict mapping nodeid to array of letters with str type
          bin_name (str): biome name in biome_timestamp format
          timestamp_list (numpy.ndarray, optional): 1D array of int Time stamps to consider (Default value = None)
          t0 (int, optional): Start time (Default value = None)
          t1 (int, optional): End time (Default value = None)

        Returns:
          dict[int,float]: dict mapping nodeid to  dequantized values of float type

        &#34;&#34;&#34;

        biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

        if timestamp_list is None:
            if (t0 is None) and (t1 is None):
                t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
                t1 = int(bin_name.split(&#39;_&#39;)[-1])+1
        R={}
        for k in dict_id_reached_by_edgelabel:
            v = dict_id_reached_by_edgelabel[k]
            R[k]=np.median(
                np.array([self.deQuantizer(
                    str(x).strip(),
                    biome_prefix,
                    timestamp_list=timestamp_list,
                    time_start=t0,
                    time_end=t1) for x in v]))
        return R


    def getNumeric_at_leaf(self,
                    Probability_distribution_dict,
                    Sample_fraction,
                    timestamp_list=None,
                    t0=None,
                    t1=None):
        &#34;&#34;&#34;Dequantize labels on graph leaf nodes to return mean and sample standard deviation of outputs

        Args:
          Probability_distribution_dict (dict[int, numpy.ndarray[float]]): dict mapping nodeid to probability distribution over output labels at that leaf node
          Sample_fraction (dict[int,float]): dict mapping nodeids to sample fraction captured by that leaf node
          timestamp_list (numpy.ndarray[int], optional): 1D array of int. Time stamps to consider (Default value = None)
          t0 (int, optional): start time (Default value = None)
          t1 (int, optional): end time (Default value = None)

        Returns:
          float,float: mean and sample standard deviation

        &#34;&#34;&#34;
        bin_name=self.TGT
        biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

        if timestamp_list is None:
            if (t0 is None) and (t1 is None):
                t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
                t1 = int(bin_name.split(&#39;_&#39;)[-1])+1

        # ----------------------------------------
        # Q is 1D array of dequantized values
        # corresponding to levels for TGT
        # ----------------------------------------
        Q=np.array([self.deQuantizer(
            str(x).strip(),
            biome_prefix,
            timestamp_list=timestamp_list,
            time_start=t0,
            time_end=t1)
                    for x in self.LABELS.keys()]).reshape(
                            len(self.LABELS),1)

        mux=0
        varx=0
        for k in Probability_distribution_dict:
            p = Probability_distribution_dict[k]

            mu_k=np.dot(p.transpose(),Q)
            var_k=np.dot(p.transpose(),(Q*Q))-mu_k*mu_k

            mux = mux + Sample_fraction[k]*mu_k
            varx = varx + Sample_fraction[k]*var_k
        return mux,np.sqrt(varx/self.total_samples)


    def regularize_distribution(self,prob,l,e=0.005):
        &#34;&#34;&#34;Regularize probability distribution
           using exponential decay to map non-detailed output of a
           single maximum likelihood label to a probability distribution.
           Used when detailed output is not available.

        Args:
          prob (float): probability of single output label of type str
          e (float, optional): small value to regularize return probability of 1.0 (Default value = 0.005)
          l (str): output label

        Returns:
          numpy.ndarray: probability distribution

        &#34;&#34;&#34;
        labels=np.array(list(self.LABELS.keys()))
        yy=np.ones(len(labels))*((1-prob-e)/(len(labels)-1))
        yy[np.where(labels==l)[0][0]]=prob-e
        dy=pd.DataFrame(yy).ewm(alpha=.8).mean()
        dy=dy/dy.sum()
        return dy.values


    def leaf_output_on_subgraph(self,nodeset):
        &#34;&#34;&#34;Find the mean and sample standard deviation of output
           in leafnodes reachable from nodeset, along with fraction of samples
           captures by this subgraph

        Args:
          nodeset (numpy.ndarray): 1D array of nodeids

        Returns:
          tuple(float,float), float: mean, sample standard deviation and sample fraction

        &#34;&#34;&#34;

        ## cLeaf is the set of leaf nodes reachable from nodeset
        # oLabels is the output labels for target and
        # is a dict mapping leafnode id to output label letter
        #
        # frac and prob are sample fraction and probability of
        # output label in leaf node, parsed from dotfile
        #
        # SUM is the total sample fraction captured by nodeset
        cLeaf=[x for x in nodeset
               if self.decision_tree.out_degree(x)==0
               and self.decision_tree.in_degree(x)==1]
        oLabels={k:str(v.split(&#39;\n&#39;)[0])
                 for (k,v) in self.tree_labels.items()
                 if k in cLeaf}

        frac={k:float(v.split(&#39;\n&#39;)[2].replace(&#39;Frac:&#39;,&#39;&#39;))
              for (k,v) in self.tree_labels.items()
              if k in cLeaf}
        if not self.detailed_labels:
            prob={k:float(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
                  for (k,v) in self.tree_labels.items()
                  if k in cLeaf}

            ## Get a kernel based distribution here.
            # self.alphabet=[&#39;A&#39;,...,&#39;E&#39;]
            # prob is regularize_distributioned to get a dict {nodeid: [p1,..,pm]}
            prob__={k:self.regularize_distribution(prob[k],oLabels[k])
                    for k in prob}
            prob=prob__
        else:
            prob={k:self.get_vector_from_dict(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
                  for (k,v) in self.tree_labels.items()
                  if k in cLeaf}

        SUM=np.array(frac.values()).sum()

        ## mean and sample estimate of standard deviation
        mu_X,sigma_X=self.getNumeric_at_leaf(prob,frac)
        return (mu_X,sigma_X),SUM


    def getHypothesisSlice(self,nid):
        &#34;&#34;&#34;Generating impact of node nid with source label prefix. Note that there can be multiple
           nodes in the tree with label that match with the source label prefix.

        Args:
          nid (int): nodeid

        Returns:
          [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): dataframe of hypotheses fragment with xvalue, ymean and y std dev

        &#34;&#34;&#34;

        cNodes=list(nx.descendants(
            self.decision_tree,nid))
        nextNodes=nx.neighbors(
            self.decision_tree,nid)

        nextedge={}
        edgeProp={}
        SUM=0.
        for nn in list(nextNodes):
            nextedge[nn]=[str(x) for x in
                          self.tree_edgelabels[(
                              nid,nn)].split(&#39;\\n&#39;)]
            if len(list(nx.descendants(
                    self.decision_tree,nn))) == 0:
                res,s=self.leaf_output_on_subgraph([nn])
            else:
                res,s=self.leaf_output_on_subgraph(list(
                    nx.descendants(self.decision_tree,nn)))
            edgeProp[nn]=res
            SUM=SUM+list(s)[0]

        # nextedge is dict: {nodeid nn: letter array by which child nn is reached}
        num_nextedge=self.getNumeric_internal(
            nextedge,
            bin_name
            =self.tree_labels[str(
                nid)])
        for (k,v) in edgeProp.items():
            num_nextedge[k]=np.append(
                num_nextedge[k],[v[0],v[1]])

        RF=pd.DataFrame(num_nextedge)
        RF.index=[&#39;x&#39;, &#39;y&#39;,&#39;sigmay&#39;]
        return RF


    def createTargetList(self,
                      source,
                      target,
                      time_end=None,
                      time_start=None):
        &#34;&#34;&#34;Create list of decision trees available within time points in model_path

        Args:
          source (str): source name in biome_timestamp format
          target (str): target name in biome_timestamp format
          time_end (int, optional): start time (Default value = None)
          time_start (int, optional): end time (Default value = None)

        Returns:
          list[str]: list of paths to decision tree models in qnet

        &#34;&#34;&#34;
        self.TGT = target
        self.SRC = source

        if self.model_path is not None:
            decision_trees = glob.glob(
                os.path.join(self.model_path,
                             target)+&#39;*.dot&#39;)
            decision_trees_ = [x for x in decision_trees
                               if (time_start
                                   &lt;= float(re.split(
                                       r&#39;(.*)_(\d+)&#39;,x)[-2])
                                   &lt;= time_end)]
        else:
            raise Exception(&#39;self.model_path is not set&#39;)
        return decision_trees_


    def get_lowlevel(self,
            source,
            target,
            time_end=None,
            time_start=None):
        &#34;&#34;&#34;Low level evaluation call to estimate local marginal regulation  \( \\alpha \)

        Args:
          source (str): source
          target (str): target
          time_end (int, optional): end time (Default value = None)
          time_start (int, optional): start time (Default value = None)

        Returns:

        &#34;&#34;&#34;
        self.TGT = target
        self.SRC = source

        decision_trees = self.createTargetList(
            source,
            target,
            time_end,
            time_start)
        grad_=[]
        # can we do this in parallel
        for tree in decision_trees:
            self.TGT = os.path.basename(tree).replace(&#39;.dot&#39;,&#39;&#39;)
            gv = pgv.AGraph(tree,
                            strict=False,
                            directed=True)

            self.decision_tree = nx.DiGraph(gv)
            self.time_start = time_start
            self.time_end = time_end

            self.tree_labels = nx.get_node_attributes(
                self.decision_tree,&#39;label&#39;)
            self.tree_edgelabels = nx.get_edge_attributes(
                self.decision_tree,&#34;label&#34;)

            nodes_with_src=[]
            for (k,v) in self.tree_labels.items():
                if self.SRC in v:
                    if self.src_time_constraint(v,self.TGT):
                        nodes_with_src=nodes_with_src+[k]

            if len(nodes_with_src)==0:
                continue

            RES=pd.concat([self.getHypothesisSlice(i).transpose()
                           for i in nodes_with_src])

            grad,pvalue=self.getAlpha(RES)
            #RES.to_csv(&#39;tmp.csv&#39;)
            #if RES.index.size &gt; 2:
            #    quit()

            #grad=stats.linregress(
            #    RES.x_.values,
            #    RES.muy.values).slope

            if np.isnan(grad):
                warnings.warn(
                    &#34;Nan encountered in causal inferrence&#34;)
                grad=np.median(
                    RES.y.values)/np.median(
                        RES.x.values)

            ns_ = re.split(r&#39;(.*)_(\d+)&#39;, self.TGT)
            self.hypotheses = self.hypotheses.append(
                {&#39;src&#39;:self.SRC,
                 &#39;tgt&#39;:&#39;&#39;.join(ns_[:-2]),
                 &#39;time_tgt&#39;:float(ns_[-2]),
                 &#39;lomar&#39;:float(grad),
                 &#39;pvalue&#39;:pvalue},
                ignore_index = True)
        return


    def get(self,
            source=None,
            target=None,
            time_end=None,
            time_start=None):
        &#34;&#34;&#34;Calculate local marginal regulation  \( \\alpha \). When source or target is not specified, we calculate for all entities available on model path. Populates self.hypotheses.

        Args:
          source (str, optional): source (Default value = None)
          target (str, optional): target (Default value = None)
          time_end (int, optional): end time (Default value = None)
          time_start (int optional): start time (Default value = None)

        Returns:

        &#34;&#34;&#34;

        if source is None:
            source = self.biomes
        else:
            if isinstance(source,str):
                source=[source]
        if target is None:
            target = self.biomes
        else:
            if isinstance(target,str):
                target=[target]

        for tgt_biome_ in tqdm(target):
            for src_biome_ in source:
                if (src_biome_ == tgt_biome_) and self.no_self_loops:
                    continue
                self.get_lowlevel(src_biome_,
                          tgt_biome_,
                          time_end=time_end,
                          time_start=time_start)

        if self.no_self_loops:
            self.hypotheses=self.hypotheses[~(self.hypotheses.src==self.hypotheses.tgt)]

        return


    def to_csv(self, *args, **kwargs):
        &#34;&#34;&#34;Output csv of hypotheses inferred. Arguments are passed to pandas.DataFrame.to_csv()

        Args:
          *args: optional arguments to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)
          **kwargs: optional keywords to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)

        Returns:

        &#34;&#34;&#34;
        self.hypotheses.to_csv(*args, **kwargs)


    def to_dot(self,filename=&#39;tmp.dot&#39;,
               hypotheses=None,
               square_mat=False):
        &#34;&#34;&#34;Output dot file of hypotheses inferred.

        Args:
          filename (str, optional): filename of dot outpt (Default value = &#39;tmp.dot&#39;)
          hypotheses ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), optional): If provided use this instead of self.hypotheses (Default value = None)
          square_mat (bool, optional): If True resturn a heatmap matrix as filename+&#39;sq.csv&#39; (Default value = False)

        Returns:

        &#34;&#34;&#34;

        if hypotheses is None:
            df=self.hypotheses.copy()
        else:
            df=hypotheses

        df=df.groupby([&#39;src&#39;,&#39;tgt&#39;]).median().reset_index()
        df=df.pivot(index=&#39;src&#39;,columns=&#39;tgt&#39;,values=&#39;lomar&#39;)
        df=df.fillna(0)

        index = df.index.union(df.columns)
        df = df.reindex(index=index, columns=index, fill_value=0)

        df=df.sort_index()
        if square_mat:
            df.to_csv(filename.replace(&#39;.dot&#39;,&#39;sq.csv&#39;))

        G = nx.from_pandas_adjacency(df,create_using=nx.DiGraph())

        from networkx.drawing.nx_agraph import write_dot
        write_dot(G,filename)

        return


    def getAlpha(self,dataframe_x_y_sigmay,N=500):
        &#34;&#34;&#34;Carryout regression to estimate   \( \\alpha \). Given mean and variance of each y observation, we
           increase the number of pints by drawing N samples from a normal distribution of mean y and std dev sigma_y.
           The slope and p-value of a linear regression fit is returned

        Args:
          dataframe_x_y_sigmax (pandas DataFrame): columns x,y,sigmay
          N (int): number of samples drawn for each x to set up regression problem

        Returns:
          float,float: slope and p-value of fit

        &#34;&#34;&#34;
        gf=pd.DataFrame(np.random.normal
                        (dataframe_x_y_sigmay.y,
                         dataframe_x_y_sigmay.sigmay,
                         [N,dataframe_x_y_sigmay.index.size]))

        RES=[dataframe_x_y_sigmay[[&#39;y&#39;,&#39;x&#39;]]]
        for i in gf.columns:
            xf=pd.DataFrame(gf.iloc[:,i])
            xf.columns=[&#39;y&#39;]
            xf[&#39;x&#39;]=dataframe_x_y_sigmay.x[i]
            RES.append(xf)
        RES=pd.concat(RES).dropna()
        
        if RES.x.max() == RES.x.min():
            return 0,1.0


        lr=stats.linregress(RES.x,RES.y)
        return lr.slope,lr.pvalue


    def get_vector_from_dict(self,str_alph_val):
        &#34;&#34;&#34;Calculate a probability distribution from string representation of
           alphabet : value read from decision tree models

        &#34;&#34;&#34;
        vec_alph_val=str_alph_val.split()

        dict_label_float={}

        for x in vec_alph_val:
            y=x.split(&#39;:&#39;)
            dict_label_float[y[0]]=float(y[1])

        prob_dist = np.zeros(len(self.LABELS.keys()))
        for i in dict_label_float:
            prob_dist[self.LABELS[i]] = dict_label_float[i]


        return prob_dist/prob_dist.sum()


    def trim_hypothesis(self,alternate_hypothesis_dataframe):
        &#34;&#34;&#34;Compate current hypothesis dataframe with alternate_hypothesis_dataframe

        Args:
          alternate_hypothesis_dataframe ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)): alternate dataframe

        Returns:
          [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): manipulated dataframe

        &#34;&#34;&#34;
        df=self.hypotheses.copy()

        #df.set_index([&#39;src&#39;,&#39;tgt&#39;]).merge


        return df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="qbiome.hypothesis.Hypothesis"><code class="flex name class">
<span>class <span class="ident">Hypothesis</span></span>
<span>(</span><span>qnet_orchestrator=None, quantizer=None, quantizer_mapfile=None, model_path=None, no_self_loops=True, causal_constraint=0, total_samples=100, detailed_labels=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate and analyze hypotheses from models inferred.
Assume biomes_timestamp format is biome_timestamp.
Also assume that dotname or decision tree name format is biome_timestamp.dot</p>
<p>Mathematical model of causal hypothesis:</p>
<hr>
<p><code>Local Marginal Regulation Coefficient: Aiming to estimate the up-regulatory/down-regulatory influence of a source organism/entity on a target organism/entity, where regulation effects are causally localized in time (future cannot affect the past) with limited memory, and potential confounding effects from other entities/organisms are marginalized out.</code></p>
<p>Let us assume a general dependency between stochastic processes <span><span class="MathJax_Preview">\nu,u, \omega </span><script type="math/tex">\nu,u, \omega </script></span> :</p>
<p><span><span class="MathJax_Preview"> \nu_t = \phi(u_{\leftarrow t},\omega_{\leftarrow t}) </span><script type="math/tex; mode=display"> \nu_t = \phi(u_{\leftarrow t},\omega_{\leftarrow t}) </script></span></p>
<p>We estimate the sign of <span><span class="MathJax_Preview"> \alpha_t</span><script type="math/tex"> \alpha_t</script></span> in a locally linear marginalized relationship <span><span class="MathJax_Preview"> \nu_t = \alpha_t u_{t'} + c </span><script type="math/tex"> \nu_t = \alpha_t u_{t'} + c </script></span> with <span><span class="MathJax_Preview">t' \in [ t-\delta, t] </span><script type="math/tex">t' \in [ t-\delta, t] </script></span> as follows:</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>qnet_orchestrator</code></strong> :&ensp;<code>qbiome.QnetOrchestrator</code></dt>
<dd>instance of qbiome.QnetOrchestrator with trained qnet model</dd>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>ath to directory containing generated decision trees in dot format (Default value = None)</dd>
<dt><strong><code>no_self_loops</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True do not report self-loops in hypotheses
(Default value = True)</dd>
<dt><strong><code>causal_constraint</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>lag of source inputs from target effects. &gt;= 0 is causal
(Default value = 0)</dd>
<dt><strong><code>total_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>total number of samples used to construct decision model
(Default value = 100)</dd>
<dt><strong><code>detailed_labels</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>if True, decision tree models have detailed output
(Default value = False)</dd>
<dt><strong><code>MAPNAME</code></strong> :&ensp;<code>str</code></dt>
<dd>path to dequantization map</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Hypothesis(object):
    &#34;&#34;&#34;Generate and analyze hypotheses from models inferred.  Assume biomes_timestamp format is biome_timestamp.
       Also assume that dotname or decision tree name format is biome_timestamp.dot

       Mathematical model of causal hypothesis:

       ---

       ```
       Local Marginal Regulation Coefficient: Aiming to estimate the up-regulatory/down-regulatory influence of a source organism/entity on a target organism/entity, where regulation effects are causally localized in time (future cannot affect the past) with limited memory, and potential confounding effects from other entities/organisms are marginalized out.
       ```

       Let us assume a general dependency between stochastic processes \(\\nu,u, \omega \) :

       $$ \\nu_t = \\phi(u_{\leftarrow t},\omega_{\leftarrow t}) $$

      We estimate the sign of \( \\alpha_t\) in a locally linear marginalized relationship \( \\nu_t = \\alpha_t u_{t&#39;} + c \) with \(t&#39; \in [ t-\delta, t] \) as follows:

    Attributes:
       qnet_orchestrator (qbiome.QnetOrchestrator): instance of qbiome.QnetOrchestrator with trained qnet model
       model_path (str, optional): ath to directory containing generated decision trees in dot format (Default value = None)
       no_self_loops (bool, optional): If True do not report self-loops in hypotheses  (Default value = True)
       causal_constraint (float, optional): lag of source inputs from target effects. &gt;= 0 is causal  (Default value = 0)
       total_samples (int, optional): total number of samples used to construct decision model  (Default value = 100)
       detailed_labels (bool, optional): if True, decision tree models have detailed output  (Default value = False)
       MAPNAME (str): path to dequantization map

    &#34;&#34;&#34;

####    def __init__(self,qnet_orchestrator,
#    model_path=None,
#    no_self_loops=True,
#    causal_constraint=0,
#    total_samples=100,
#    detailed_labels=False):

    def __init__(self,
                 qnet_orchestrator=None,
                 quantizer=None,
                 quantizer_mapfile=None,
                 model_path=None,
                 no_self_loops=True,
                 causal_constraint=0,
                 total_samples=100,
                 detailed_labels=False):
        &#34;&#34;&#34;

        &#34;&#34;&#34;

        self.time_start = None
        self.time_end = None

        self.model_path = model_path
        self.qnet_orchestrator = qnet_orchestrator
        self.quantizer = quantizer


        if all(v is None for v in[qnet_orchestrator,quantizer,quantizer_mapfile]):
            raise Exception(&#39;Either qnet_orchestrator or quantizer or quantizer_mapfile must be provided to Hypothesis&#39;)

        if self.qnet_orchestrator is not None:
            self.quantizer = self.qnet_orchestrator.quantizer
        if self.quantizer is not None:
            self.variable_bin_map = self.quantizer.variable_bin_map
        else:
            self.variable_bin_map = np.load(quantizer_mapfile, allow_pickle=True)

        self.biomes_timestamp = [x for x in self.variable_bin_map.keys()]

        self.biomes = list(set([&#39;_&#39;.join(x.split(&#39;_&#39;)[:-1])
                                for x in self.biomes_timestamp]))

        self.NMAP = self.variable_bin_map

        if self.quantizer is not None:
            self.LABELS = quantizer.labels
        else:
            warnings.warn(&#34;Using manually coded labels. Provide quantizer to Hypothesis&#34;)
            self.LABELS = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2, &#39;D&#39;:3, &#39;E&#39;:4}

        self.total_samples = total_samples
        self.detailed_labels = detailed_labels

        self.decision_tree = None
        self.tree_labels = None
        self.tree_edgelabels = None
        self.TGT = None
        self.SRC = None
        self.no_self_loops = no_self_loops

        self.causal_constraint = causal_constraint
        self.hypotheses=pd.DataFrame(columns=[&#39;src&#39;,&#39;tgt&#39;,&#39;time_tgt&#39;,&#39;lomar&#39;,&#39;pvalue&#39;])


    def src_time_constraint(self,source,target):
        &#34;&#34;&#34;Constrain which time points for source can be
           considered as inputs when computing source to target influences.
           If causal_constraint is None, there are no restrictions.
           Otherwise, only sources that are at least causal_constraint
           units of time prior to the target may be considered. Default is 0.
           Negative values are possible.

        Args:
          source (str): full source name in biome_timestamp format
          target (str): full target name in biome_timestamp format

        Returns:
          bool: Truth value of src acceptance

        &#34;&#34;&#34;
        _, source_biome_full, source_timestamp, _ \
            = re.split(r&#39;(.*)_(\d+)&#39;, source)
        _, target_biome_full, target_timestamp, _ \
            = re.split(r&#39;(.*)_(\d+)&#39;, target)

        if self.causal_constraint is not None:
            if (float(source_timestamp)
            + self.causal_constraint
                &lt;= float(target_timestamp)):
                return True
            return False
        return True


    def deQuantize_lowlevel(self,
                    letter,
                    bin_arr):
        &#34;&#34;&#34;Low level dequantizer function

        Args:
          letter (str): quantized level (str) or nan
          bin_arr (numpy.ndarray): 1D array of floats, which are quantization levels from trained quantizer.

        Returns:
          float: dequantized value

        &#34;&#34;&#34;

        if letter is np.nan or letter == &#39;nan&#39;:
            return np.nan
        lo = self.LABELS[letter]
        hi = lo + 1
        val = (bin_arr[lo] + bin_arr[hi]) / 2
        return val


    def deQuantizer(self,
                    letter,
                    biome_prefix,
                    timestamp_list=None,
                    time_start=None,
                    time_end=None):
        &#34;&#34;&#34;Dequantizer function calling low level deQuantize_lowlevel to account
           for the possibility of multiple timestamps being averaged over,
           and ability to operate with incomplete biome names.

        Args:
          letter (str): quantized level (str) or nan
          biome_prefix (str): prefix of biome name
          timestamp_list (numpy.ndarray, optional): 1D array of int time-stamps to consider. (Default value = None)
          time_start (int, optional): Start time. (Default value = None)
          time_end (int, optional): End time. (Default value = None)

        Returns:
          float: median of dequantized value

        &#34;&#34;&#34;
        vals = []
        if timestamp_list is None:
            if time_start is None and self.time_end is None:
                # average over all
                for biome_key in self.NMAP:
                    if biome_prefix in biome_key:
                        vals.append(
                            self.deQuantize_lowlevel(letter,
                                             self.NMAP[biome_key]))
            elif time_start is None:
                time_end = int(time_end)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ \
                        = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time &lt;= time_end) and (biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
            elif time_end is None:
                time_start = int(time_start)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time_start &lt;= time) and (biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
            else: # both present
                time_start = int(time_start)
                time_end = int(time_end)
                for biome_key in self.NMAP:
                    _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                    time = int(time)
                    if (time_start &lt;= time &lt;= time_end) and (
                            biome_prefix in biome_key):
                        vals.append(
                            self.deQuantize_lowlevel(letter,
                                             self.NMAP[biome_key]))
        else:
            for biome_key in self.NMAP:
                _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                time = int(time)
                if time in timestamp_list:
                    vals.append(self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))

        return np.median(vals)


    def getNumeric_internal(self,
               dict_id_reached_by_edgelabel,
               bin_name,
               timestamp_list=None,
               t0=None,
               t1=None):
        &#34;&#34;&#34;Dequantize labels on graph non-leaf nodes

        Args:
          dict_id_reached_by_edgelabel (dict[int,list[str]]): dict mapping nodeid to array of letters with str type
          bin_name (str): biome name in biome_timestamp format
          timestamp_list (numpy.ndarray, optional): 1D array of int Time stamps to consider (Default value = None)
          t0 (int, optional): Start time (Default value = None)
          t1 (int, optional): End time (Default value = None)

        Returns:
          dict[int,float]: dict mapping nodeid to  dequantized values of float type

        &#34;&#34;&#34;

        biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

        if timestamp_list is None:
            if (t0 is None) and (t1 is None):
                t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
                t1 = int(bin_name.split(&#39;_&#39;)[-1])+1
        R={}
        for k in dict_id_reached_by_edgelabel:
            v = dict_id_reached_by_edgelabel[k]
            R[k]=np.median(
                np.array([self.deQuantizer(
                    str(x).strip(),
                    biome_prefix,
                    timestamp_list=timestamp_list,
                    time_start=t0,
                    time_end=t1) for x in v]))
        return R


    def getNumeric_at_leaf(self,
                    Probability_distribution_dict,
                    Sample_fraction,
                    timestamp_list=None,
                    t0=None,
                    t1=None):
        &#34;&#34;&#34;Dequantize labels on graph leaf nodes to return mean and sample standard deviation of outputs

        Args:
          Probability_distribution_dict (dict[int, numpy.ndarray[float]]): dict mapping nodeid to probability distribution over output labels at that leaf node
          Sample_fraction (dict[int,float]): dict mapping nodeids to sample fraction captured by that leaf node
          timestamp_list (numpy.ndarray[int], optional): 1D array of int. Time stamps to consider (Default value = None)
          t0 (int, optional): start time (Default value = None)
          t1 (int, optional): end time (Default value = None)

        Returns:
          float,float: mean and sample standard deviation

        &#34;&#34;&#34;
        bin_name=self.TGT
        biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

        if timestamp_list is None:
            if (t0 is None) and (t1 is None):
                t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
                t1 = int(bin_name.split(&#39;_&#39;)[-1])+1

        # ----------------------------------------
        # Q is 1D array of dequantized values
        # corresponding to levels for TGT
        # ----------------------------------------
        Q=np.array([self.deQuantizer(
            str(x).strip(),
            biome_prefix,
            timestamp_list=timestamp_list,
            time_start=t0,
            time_end=t1)
                    for x in self.LABELS.keys()]).reshape(
                            len(self.LABELS),1)

        mux=0
        varx=0
        for k in Probability_distribution_dict:
            p = Probability_distribution_dict[k]

            mu_k=np.dot(p.transpose(),Q)
            var_k=np.dot(p.transpose(),(Q*Q))-mu_k*mu_k

            mux = mux + Sample_fraction[k]*mu_k
            varx = varx + Sample_fraction[k]*var_k
        return mux,np.sqrt(varx/self.total_samples)


    def regularize_distribution(self,prob,l,e=0.005):
        &#34;&#34;&#34;Regularize probability distribution
           using exponential decay to map non-detailed output of a
           single maximum likelihood label to a probability distribution.
           Used when detailed output is not available.

        Args:
          prob (float): probability of single output label of type str
          e (float, optional): small value to regularize return probability of 1.0 (Default value = 0.005)
          l (str): output label

        Returns:
          numpy.ndarray: probability distribution

        &#34;&#34;&#34;
        labels=np.array(list(self.LABELS.keys()))
        yy=np.ones(len(labels))*((1-prob-e)/(len(labels)-1))
        yy[np.where(labels==l)[0][0]]=prob-e
        dy=pd.DataFrame(yy).ewm(alpha=.8).mean()
        dy=dy/dy.sum()
        return dy.values


    def leaf_output_on_subgraph(self,nodeset):
        &#34;&#34;&#34;Find the mean and sample standard deviation of output
           in leafnodes reachable from nodeset, along with fraction of samples
           captures by this subgraph

        Args:
          nodeset (numpy.ndarray): 1D array of nodeids

        Returns:
          tuple(float,float), float: mean, sample standard deviation and sample fraction

        &#34;&#34;&#34;

        ## cLeaf is the set of leaf nodes reachable from nodeset
        # oLabels is the output labels for target and
        # is a dict mapping leafnode id to output label letter
        #
        # frac and prob are sample fraction and probability of
        # output label in leaf node, parsed from dotfile
        #
        # SUM is the total sample fraction captured by nodeset
        cLeaf=[x for x in nodeset
               if self.decision_tree.out_degree(x)==0
               and self.decision_tree.in_degree(x)==1]
        oLabels={k:str(v.split(&#39;\n&#39;)[0])
                 for (k,v) in self.tree_labels.items()
                 if k in cLeaf}

        frac={k:float(v.split(&#39;\n&#39;)[2].replace(&#39;Frac:&#39;,&#39;&#39;))
              for (k,v) in self.tree_labels.items()
              if k in cLeaf}
        if not self.detailed_labels:
            prob={k:float(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
                  for (k,v) in self.tree_labels.items()
                  if k in cLeaf}

            ## Get a kernel based distribution here.
            # self.alphabet=[&#39;A&#39;,...,&#39;E&#39;]
            # prob is regularize_distributioned to get a dict {nodeid: [p1,..,pm]}
            prob__={k:self.regularize_distribution(prob[k],oLabels[k])
                    for k in prob}
            prob=prob__
        else:
            prob={k:self.get_vector_from_dict(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
                  for (k,v) in self.tree_labels.items()
                  if k in cLeaf}

        SUM=np.array(frac.values()).sum()

        ## mean and sample estimate of standard deviation
        mu_X,sigma_X=self.getNumeric_at_leaf(prob,frac)
        return (mu_X,sigma_X),SUM


    def getHypothesisSlice(self,nid):
        &#34;&#34;&#34;Generating impact of node nid with source label prefix. Note that there can be multiple
           nodes in the tree with label that match with the source label prefix.

        Args:
          nid (int): nodeid

        Returns:
          [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): dataframe of hypotheses fragment with xvalue, ymean and y std dev

        &#34;&#34;&#34;

        cNodes=list(nx.descendants(
            self.decision_tree,nid))
        nextNodes=nx.neighbors(
            self.decision_tree,nid)

        nextedge={}
        edgeProp={}
        SUM=0.
        for nn in list(nextNodes):
            nextedge[nn]=[str(x) for x in
                          self.tree_edgelabels[(
                              nid,nn)].split(&#39;\\n&#39;)]
            if len(list(nx.descendants(
                    self.decision_tree,nn))) == 0:
                res,s=self.leaf_output_on_subgraph([nn])
            else:
                res,s=self.leaf_output_on_subgraph(list(
                    nx.descendants(self.decision_tree,nn)))
            edgeProp[nn]=res
            SUM=SUM+list(s)[0]

        # nextedge is dict: {nodeid nn: letter array by which child nn is reached}
        num_nextedge=self.getNumeric_internal(
            nextedge,
            bin_name
            =self.tree_labels[str(
                nid)])
        for (k,v) in edgeProp.items():
            num_nextedge[k]=np.append(
                num_nextedge[k],[v[0],v[1]])

        RF=pd.DataFrame(num_nextedge)
        RF.index=[&#39;x&#39;, &#39;y&#39;,&#39;sigmay&#39;]
        return RF


    def createTargetList(self,
                      source,
                      target,
                      time_end=None,
                      time_start=None):
        &#34;&#34;&#34;Create list of decision trees available within time points in model_path

        Args:
          source (str): source name in biome_timestamp format
          target (str): target name in biome_timestamp format
          time_end (int, optional): start time (Default value = None)
          time_start (int, optional): end time (Default value = None)

        Returns:
          list[str]: list of paths to decision tree models in qnet

        &#34;&#34;&#34;
        self.TGT = target
        self.SRC = source

        if self.model_path is not None:
            decision_trees = glob.glob(
                os.path.join(self.model_path,
                             target)+&#39;*.dot&#39;)
            decision_trees_ = [x for x in decision_trees
                               if (time_start
                                   &lt;= float(re.split(
                                       r&#39;(.*)_(\d+)&#39;,x)[-2])
                                   &lt;= time_end)]
        else:
            raise Exception(&#39;self.model_path is not set&#39;)
        return decision_trees_


    def get_lowlevel(self,
            source,
            target,
            time_end=None,
            time_start=None):
        &#34;&#34;&#34;Low level evaluation call to estimate local marginal regulation  \( \\alpha \)

        Args:
          source (str): source
          target (str): target
          time_end (int, optional): end time (Default value = None)
          time_start (int, optional): start time (Default value = None)

        Returns:

        &#34;&#34;&#34;
        self.TGT = target
        self.SRC = source

        decision_trees = self.createTargetList(
            source,
            target,
            time_end,
            time_start)
        grad_=[]
        # can we do this in parallel
        for tree in decision_trees:
            self.TGT = os.path.basename(tree).replace(&#39;.dot&#39;,&#39;&#39;)
            gv = pgv.AGraph(tree,
                            strict=False,
                            directed=True)

            self.decision_tree = nx.DiGraph(gv)
            self.time_start = time_start
            self.time_end = time_end

            self.tree_labels = nx.get_node_attributes(
                self.decision_tree,&#39;label&#39;)
            self.tree_edgelabels = nx.get_edge_attributes(
                self.decision_tree,&#34;label&#34;)

            nodes_with_src=[]
            for (k,v) in self.tree_labels.items():
                if self.SRC in v:
                    if self.src_time_constraint(v,self.TGT):
                        nodes_with_src=nodes_with_src+[k]

            if len(nodes_with_src)==0:
                continue

            RES=pd.concat([self.getHypothesisSlice(i).transpose()
                           for i in nodes_with_src])

            grad,pvalue=self.getAlpha(RES)
            #RES.to_csv(&#39;tmp.csv&#39;)
            #if RES.index.size &gt; 2:
            #    quit()

            #grad=stats.linregress(
            #    RES.x_.values,
            #    RES.muy.values).slope

            if np.isnan(grad):
                warnings.warn(
                    &#34;Nan encountered in causal inferrence&#34;)
                grad=np.median(
                    RES.y.values)/np.median(
                        RES.x.values)

            ns_ = re.split(r&#39;(.*)_(\d+)&#39;, self.TGT)
            self.hypotheses = self.hypotheses.append(
                {&#39;src&#39;:self.SRC,
                 &#39;tgt&#39;:&#39;&#39;.join(ns_[:-2]),
                 &#39;time_tgt&#39;:float(ns_[-2]),
                 &#39;lomar&#39;:float(grad),
                 &#39;pvalue&#39;:pvalue},
                ignore_index = True)
        return


    def get(self,
            source=None,
            target=None,
            time_end=None,
            time_start=None):
        &#34;&#34;&#34;Calculate local marginal regulation  \( \\alpha \). When source or target is not specified, we calculate for all entities available on model path. Populates self.hypotheses.

        Args:
          source (str, optional): source (Default value = None)
          target (str, optional): target (Default value = None)
          time_end (int, optional): end time (Default value = None)
          time_start (int optional): start time (Default value = None)

        Returns:

        &#34;&#34;&#34;

        if source is None:
            source = self.biomes
        else:
            if isinstance(source,str):
                source=[source]
        if target is None:
            target = self.biomes
        else:
            if isinstance(target,str):
                target=[target]

        for tgt_biome_ in tqdm(target):
            for src_biome_ in source:
                if (src_biome_ == tgt_biome_) and self.no_self_loops:
                    continue
                self.get_lowlevel(src_biome_,
                          tgt_biome_,
                          time_end=time_end,
                          time_start=time_start)

        if self.no_self_loops:
            self.hypotheses=self.hypotheses[~(self.hypotheses.src==self.hypotheses.tgt)]

        return


    def to_csv(self, *args, **kwargs):
        &#34;&#34;&#34;Output csv of hypotheses inferred. Arguments are passed to pandas.DataFrame.to_csv()

        Args:
          *args: optional arguments to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)
          **kwargs: optional keywords to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)

        Returns:

        &#34;&#34;&#34;
        self.hypotheses.to_csv(*args, **kwargs)


    def to_dot(self,filename=&#39;tmp.dot&#39;,
               hypotheses=None,
               square_mat=False):
        &#34;&#34;&#34;Output dot file of hypotheses inferred.

        Args:
          filename (str, optional): filename of dot outpt (Default value = &#39;tmp.dot&#39;)
          hypotheses ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), optional): If provided use this instead of self.hypotheses (Default value = None)
          square_mat (bool, optional): If True resturn a heatmap matrix as filename+&#39;sq.csv&#39; (Default value = False)

        Returns:

        &#34;&#34;&#34;

        if hypotheses is None:
            df=self.hypotheses.copy()
        else:
            df=hypotheses

        df=df.groupby([&#39;src&#39;,&#39;tgt&#39;]).median().reset_index()
        df=df.pivot(index=&#39;src&#39;,columns=&#39;tgt&#39;,values=&#39;lomar&#39;)
        df=df.fillna(0)

        index = df.index.union(df.columns)
        df = df.reindex(index=index, columns=index, fill_value=0)

        df=df.sort_index()
        if square_mat:
            df.to_csv(filename.replace(&#39;.dot&#39;,&#39;sq.csv&#39;))

        G = nx.from_pandas_adjacency(df,create_using=nx.DiGraph())

        from networkx.drawing.nx_agraph import write_dot
        write_dot(G,filename)

        return


    def getAlpha(self,dataframe_x_y_sigmay,N=500):
        &#34;&#34;&#34;Carryout regression to estimate   \( \\alpha \). Given mean and variance of each y observation, we
           increase the number of pints by drawing N samples from a normal distribution of mean y and std dev sigma_y.
           The slope and p-value of a linear regression fit is returned

        Args:
          dataframe_x_y_sigmax (pandas DataFrame): columns x,y,sigmay
          N (int): number of samples drawn for each x to set up regression problem

        Returns:
          float,float: slope and p-value of fit

        &#34;&#34;&#34;
        gf=pd.DataFrame(np.random.normal
                        (dataframe_x_y_sigmay.y,
                         dataframe_x_y_sigmay.sigmay,
                         [N,dataframe_x_y_sigmay.index.size]))

        RES=[dataframe_x_y_sigmay[[&#39;y&#39;,&#39;x&#39;]]]
        for i in gf.columns:
            xf=pd.DataFrame(gf.iloc[:,i])
            xf.columns=[&#39;y&#39;]
            xf[&#39;x&#39;]=dataframe_x_y_sigmay.x[i]
            RES.append(xf)
        RES=pd.concat(RES).dropna()
        
        if RES.x.max() == RES.x.min():
            return 0,1.0


        lr=stats.linregress(RES.x,RES.y)
        return lr.slope,lr.pvalue


    def get_vector_from_dict(self,str_alph_val):
        &#34;&#34;&#34;Calculate a probability distribution from string representation of
           alphabet : value read from decision tree models

        &#34;&#34;&#34;
        vec_alph_val=str_alph_val.split()

        dict_label_float={}

        for x in vec_alph_val:
            y=x.split(&#39;:&#39;)
            dict_label_float[y[0]]=float(y[1])

        prob_dist = np.zeros(len(self.LABELS.keys()))
        for i in dict_label_float:
            prob_dist[self.LABELS[i]] = dict_label_float[i]


        return prob_dist/prob_dist.sum()


    def trim_hypothesis(self,alternate_hypothesis_dataframe):
        &#34;&#34;&#34;Compate current hypothesis dataframe with alternate_hypothesis_dataframe

        Args:
          alternate_hypothesis_dataframe ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)): alternate dataframe

        Returns:
          [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): manipulated dataframe

        &#34;&#34;&#34;
        df=self.hypotheses.copy()

        #df.set_index([&#39;src&#39;,&#39;tgt&#39;]).merge


        return df</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="qbiome.hypothesis.Hypothesis.createTargetList"><code class="name flex">
<span>def <span class="ident">createTargetList</span></span>(<span>self, source, target, time_end=None, time_start=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Create list of decision trees available within time points in model_path</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code></dt>
<dd>source name in biome_timestamp format</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>target name in biome_timestamp format</dd>
<dt><strong><code>time_end</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>start time (Default value = None)</dd>
<dt><strong><code>time_start</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>end time (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[str]</code></dt>
<dd>list of paths to decision tree models in qnet</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def createTargetList(self,
                  source,
                  target,
                  time_end=None,
                  time_start=None):
    &#34;&#34;&#34;Create list of decision trees available within time points in model_path

    Args:
      source (str): source name in biome_timestamp format
      target (str): target name in biome_timestamp format
      time_end (int, optional): start time (Default value = None)
      time_start (int, optional): end time (Default value = None)

    Returns:
      list[str]: list of paths to decision tree models in qnet

    &#34;&#34;&#34;
    self.TGT = target
    self.SRC = source

    if self.model_path is not None:
        decision_trees = glob.glob(
            os.path.join(self.model_path,
                         target)+&#39;*.dot&#39;)
        decision_trees_ = [x for x in decision_trees
                           if (time_start
                               &lt;= float(re.split(
                                   r&#39;(.*)_(\d+)&#39;,x)[-2])
                               &lt;= time_end)]
    else:
        raise Exception(&#39;self.model_path is not set&#39;)
    return decision_trees_</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.deQuantize_lowlevel"><code class="name flex">
<span>def <span class="ident">deQuantize_lowlevel</span></span>(<span>self, letter, bin_arr)</span>
</code></dt>
<dd>
<div class="desc"><p>Low level dequantizer function</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>letter</code></strong> :&ensp;<code>str</code></dt>
<dd>quantized level (str) or nan</dd>
<dt><strong><code>bin_arr</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>1D array of floats, which are quantization levels from trained quantizer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>dequantized value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deQuantize_lowlevel(self,
                letter,
                bin_arr):
    &#34;&#34;&#34;Low level dequantizer function

    Args:
      letter (str): quantized level (str) or nan
      bin_arr (numpy.ndarray): 1D array of floats, which are quantization levels from trained quantizer.

    Returns:
      float: dequantized value

    &#34;&#34;&#34;

    if letter is np.nan or letter == &#39;nan&#39;:
        return np.nan
    lo = self.LABELS[letter]
    hi = lo + 1
    val = (bin_arr[lo] + bin_arr[hi]) / 2
    return val</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.deQuantizer"><code class="name flex">
<span>def <span class="ident">deQuantizer</span></span>(<span>self, letter, biome_prefix, timestamp_list=None, time_start=None, time_end=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dequantizer function calling low level deQuantize_lowlevel to account
for the possibility of multiple timestamps being averaged over,
and ability to operate with incomplete biome names.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>letter</code></strong> :&ensp;<code>str</code></dt>
<dd>quantized level (str) or nan</dd>
<dt><strong><code>biome_prefix</code></strong> :&ensp;<code>str</code></dt>
<dd>prefix of biome name</dd>
<dt><strong><code>timestamp_list</code></strong> :&ensp;<code>numpy.ndarray</code>, optional</dt>
<dd>1D array of int time-stamps to consider. (Default value = None)</dd>
<dt><strong><code>time_start</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Start time. (Default value = None)</dd>
<dt><strong><code>time_end</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>End time. (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>median of dequantized value</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deQuantizer(self,
                letter,
                biome_prefix,
                timestamp_list=None,
                time_start=None,
                time_end=None):
    &#34;&#34;&#34;Dequantizer function calling low level deQuantize_lowlevel to account
       for the possibility of multiple timestamps being averaged over,
       and ability to operate with incomplete biome names.

    Args:
      letter (str): quantized level (str) or nan
      biome_prefix (str): prefix of biome name
      timestamp_list (numpy.ndarray, optional): 1D array of int time-stamps to consider. (Default value = None)
      time_start (int, optional): Start time. (Default value = None)
      time_end (int, optional): End time. (Default value = None)

    Returns:
      float: median of dequantized value

    &#34;&#34;&#34;
    vals = []
    if timestamp_list is None:
        if time_start is None and self.time_end is None:
            # average over all
            for biome_key in self.NMAP:
                if biome_prefix in biome_key:
                    vals.append(
                        self.deQuantize_lowlevel(letter,
                                         self.NMAP[biome_key]))
        elif time_start is None:
            time_end = int(time_end)
            for biome_key in self.NMAP:
                _, biome_full, time, _ \
                    = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                time = int(time)
                if (time &lt;= time_end) and (biome_prefix in biome_key):
                    vals.append(
                        self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
        elif time_end is None:
            time_start = int(time_start)
            for biome_key in self.NMAP:
                _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                time = int(time)
                if (time_start &lt;= time) and (biome_prefix in biome_key):
                    vals.append(
                        self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))
        else: # both present
            time_start = int(time_start)
            time_end = int(time_end)
            for biome_key in self.NMAP:
                _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
                time = int(time)
                if (time_start &lt;= time &lt;= time_end) and (
                        biome_prefix in biome_key):
                    vals.append(
                        self.deQuantize_lowlevel(letter,
                                         self.NMAP[biome_key]))
    else:
        for biome_key in self.NMAP:
            _, biome_full, time, _ = re.split(r&#39;(.*)_(\d+)&#39;, biome_key)
            time = int(time)
            if time in timestamp_list:
                vals.append(self.deQuantize_lowlevel(letter, self.NMAP[biome_key]))

    return np.median(vals)</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.get"><code class="name flex">
<span>def <span class="ident">get</span></span>(<span>self, source=None, target=None, time_end=None, time_start=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate local marginal regulation
<span><span class="MathJax_Preview"> \alpha </span><script type="math/tex"> \alpha </script></span>. When source or target is not specified, we calculate for all entities available on model path. Populates self.hypotheses.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>source (Default value = None)</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>target (Default value = None)</dd>
<dt><strong><code>time_end</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>end time (Default value = None)</dd>
<dt><strong><code>time_start</code></strong> :&ensp;<code>int optional</code></dt>
<dd>start time (Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get(self,
        source=None,
        target=None,
        time_end=None,
        time_start=None):
    &#34;&#34;&#34;Calculate local marginal regulation  \( \\alpha \). When source or target is not specified, we calculate for all entities available on model path. Populates self.hypotheses.

    Args:
      source (str, optional): source (Default value = None)
      target (str, optional): target (Default value = None)
      time_end (int, optional): end time (Default value = None)
      time_start (int optional): start time (Default value = None)

    Returns:

    &#34;&#34;&#34;

    if source is None:
        source = self.biomes
    else:
        if isinstance(source,str):
            source=[source]
    if target is None:
        target = self.biomes
    else:
        if isinstance(target,str):
            target=[target]

    for tgt_biome_ in tqdm(target):
        for src_biome_ in source:
            if (src_biome_ == tgt_biome_) and self.no_self_loops:
                continue
            self.get_lowlevel(src_biome_,
                      tgt_biome_,
                      time_end=time_end,
                      time_start=time_start)

    if self.no_self_loops:
        self.hypotheses=self.hypotheses[~(self.hypotheses.src==self.hypotheses.tgt)]

    return</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.getAlpha"><code class="name flex">
<span>def <span class="ident">getAlpha</span></span>(<span>self, dataframe_x_y_sigmay, N=500)</span>
</code></dt>
<dd>
<div class="desc"><p>Carryout regression to estimate
<span><span class="MathJax_Preview"> \alpha </span><script type="math/tex"> \alpha </script></span>. Given mean and variance of each y observation, we
increase the number of pints by drawing N samples from a normal distribution of mean y and std dev sigma_y.
The slope and p-value of a linear regression fit is returned</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dataframe_x_y_sigmax</code></strong> :&ensp;<code>pandas DataFrame</code></dt>
<dd>columns x,y,sigmay</dd>
<dt><strong><code>N</code></strong> :&ensp;<code>int</code></dt>
<dd>number of samples drawn for each x to set up regression problem</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float,float</code></dt>
<dd>slope and p-value of fit</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getAlpha(self,dataframe_x_y_sigmay,N=500):
    &#34;&#34;&#34;Carryout regression to estimate   \( \\alpha \). Given mean and variance of each y observation, we
       increase the number of pints by drawing N samples from a normal distribution of mean y and std dev sigma_y.
       The slope and p-value of a linear regression fit is returned

    Args:
      dataframe_x_y_sigmax (pandas DataFrame): columns x,y,sigmay
      N (int): number of samples drawn for each x to set up regression problem

    Returns:
      float,float: slope and p-value of fit

    &#34;&#34;&#34;
    gf=pd.DataFrame(np.random.normal
                    (dataframe_x_y_sigmay.y,
                     dataframe_x_y_sigmay.sigmay,
                     [N,dataframe_x_y_sigmay.index.size]))

    RES=[dataframe_x_y_sigmay[[&#39;y&#39;,&#39;x&#39;]]]
    for i in gf.columns:
        xf=pd.DataFrame(gf.iloc[:,i])
        xf.columns=[&#39;y&#39;]
        xf[&#39;x&#39;]=dataframe_x_y_sigmay.x[i]
        RES.append(xf)
    RES=pd.concat(RES).dropna()
    
    if RES.x.max() == RES.x.min():
        return 0,1.0


    lr=stats.linregress(RES.x,RES.y)
    return lr.slope,lr.pvalue</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.getHypothesisSlice"><code class="name flex">
<span>def <span class="ident">getHypothesisSlice</span></span>(<span>self, nid)</span>
</code></dt>
<dd>
<div class="desc"><p>Generating impact of node nid with source label prefix. Note that there can be multiple
nodes in the tree with label that match with the source label prefix.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nid</code></strong> :&ensp;<code>int</code></dt>
<dd>nodeid</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">pandas.DataFrame</a>: dataframe of hypotheses fragment with xvalue, ymean and y std dev</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getHypothesisSlice(self,nid):
    &#34;&#34;&#34;Generating impact of node nid with source label prefix. Note that there can be multiple
       nodes in the tree with label that match with the source label prefix.

    Args:
      nid (int): nodeid

    Returns:
      [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): dataframe of hypotheses fragment with xvalue, ymean and y std dev

    &#34;&#34;&#34;

    cNodes=list(nx.descendants(
        self.decision_tree,nid))
    nextNodes=nx.neighbors(
        self.decision_tree,nid)

    nextedge={}
    edgeProp={}
    SUM=0.
    for nn in list(nextNodes):
        nextedge[nn]=[str(x) for x in
                      self.tree_edgelabels[(
                          nid,nn)].split(&#39;\\n&#39;)]
        if len(list(nx.descendants(
                self.decision_tree,nn))) == 0:
            res,s=self.leaf_output_on_subgraph([nn])
        else:
            res,s=self.leaf_output_on_subgraph(list(
                nx.descendants(self.decision_tree,nn)))
        edgeProp[nn]=res
        SUM=SUM+list(s)[0]

    # nextedge is dict: {nodeid nn: letter array by which child nn is reached}
    num_nextedge=self.getNumeric_internal(
        nextedge,
        bin_name
        =self.tree_labels[str(
            nid)])
    for (k,v) in edgeProp.items():
        num_nextedge[k]=np.append(
            num_nextedge[k],[v[0],v[1]])

    RF=pd.DataFrame(num_nextedge)
    RF.index=[&#39;x&#39;, &#39;y&#39;,&#39;sigmay&#39;]
    return RF</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.getNumeric_at_leaf"><code class="name flex">
<span>def <span class="ident">getNumeric_at_leaf</span></span>(<span>self, Probability_distribution_dict, Sample_fraction, timestamp_list=None, t0=None, t1=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dequantize labels on graph leaf nodes to return mean and sample standard deviation of outputs</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Probability_distribution_dict</code></strong> :&ensp;<code>dict[int, numpy.ndarray[float]]</code></dt>
<dd>dict mapping nodeid to probability distribution over output labels at that leaf node</dd>
<dt><strong><code>Sample_fraction</code></strong> :&ensp;<code>dict[int,float]</code></dt>
<dd>dict mapping nodeids to sample fraction captured by that leaf node</dd>
<dt><strong><code>timestamp_list</code></strong> :&ensp;<code>numpy.ndarray[int]</code>, optional</dt>
<dd>1D array of int. Time stamps to consider (Default value = None)</dd>
<dt><strong><code>t0</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>start time (Default value = None)</dd>
<dt><strong><code>t1</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>end time (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float,float</code></dt>
<dd>mean and sample standard deviation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getNumeric_at_leaf(self,
                Probability_distribution_dict,
                Sample_fraction,
                timestamp_list=None,
                t0=None,
                t1=None):
    &#34;&#34;&#34;Dequantize labels on graph leaf nodes to return mean and sample standard deviation of outputs

    Args:
      Probability_distribution_dict (dict[int, numpy.ndarray[float]]): dict mapping nodeid to probability distribution over output labels at that leaf node
      Sample_fraction (dict[int,float]): dict mapping nodeids to sample fraction captured by that leaf node
      timestamp_list (numpy.ndarray[int], optional): 1D array of int. Time stamps to consider (Default value = None)
      t0 (int, optional): start time (Default value = None)
      t1 (int, optional): end time (Default value = None)

    Returns:
      float,float: mean and sample standard deviation

    &#34;&#34;&#34;
    bin_name=self.TGT
    biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

    if timestamp_list is None:
        if (t0 is None) and (t1 is None):
            t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
            t1 = int(bin_name.split(&#39;_&#39;)[-1])+1

    # ----------------------------------------
    # Q is 1D array of dequantized values
    # corresponding to levels for TGT
    # ----------------------------------------
    Q=np.array([self.deQuantizer(
        str(x).strip(),
        biome_prefix,
        timestamp_list=timestamp_list,
        time_start=t0,
        time_end=t1)
                for x in self.LABELS.keys()]).reshape(
                        len(self.LABELS),1)

    mux=0
    varx=0
    for k in Probability_distribution_dict:
        p = Probability_distribution_dict[k]

        mu_k=np.dot(p.transpose(),Q)
        var_k=np.dot(p.transpose(),(Q*Q))-mu_k*mu_k

        mux = mux + Sample_fraction[k]*mu_k
        varx = varx + Sample_fraction[k]*var_k
    return mux,np.sqrt(varx/self.total_samples)</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.getNumeric_internal"><code class="name flex">
<span>def <span class="ident">getNumeric_internal</span></span>(<span>self, dict_id_reached_by_edgelabel, bin_name, timestamp_list=None, t0=None, t1=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Dequantize labels on graph non-leaf nodes</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dict_id_reached_by_edgelabel</code></strong> :&ensp;<code>dict[int,list[str]]</code></dt>
<dd>dict mapping nodeid to array of letters with str type</dd>
<dt><strong><code>bin_name</code></strong> :&ensp;<code>str</code></dt>
<dd>biome name in biome_timestamp format</dd>
<dt><strong><code>timestamp_list</code></strong> :&ensp;<code>numpy.ndarray</code>, optional</dt>
<dd>1D array of int Time stamps to consider (Default value = None)</dd>
<dt><strong><code>t0</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Start time (Default value = None)</dd>
<dt><strong><code>t1</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>End time (Default value = None)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict[int,float]</code></dt>
<dd>dict mapping nodeid to
dequantized values of float type</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def getNumeric_internal(self,
           dict_id_reached_by_edgelabel,
           bin_name,
           timestamp_list=None,
           t0=None,
           t1=None):
    &#34;&#34;&#34;Dequantize labels on graph non-leaf nodes

    Args:
      dict_id_reached_by_edgelabel (dict[int,list[str]]): dict mapping nodeid to array of letters with str type
      bin_name (str): biome name in biome_timestamp format
      timestamp_list (numpy.ndarray, optional): 1D array of int Time stamps to consider (Default value = None)
      t0 (int, optional): Start time (Default value = None)
      t1 (int, optional): End time (Default value = None)

    Returns:
      dict[int,float]: dict mapping nodeid to  dequantized values of float type

    &#34;&#34;&#34;

    biome_prefix = &#39;_&#39;.join(bin_name.split(&#39;_&#39;)[:-1])

    if timestamp_list is None:
        if (t0 is None) and (t1 is None):
            t0 = int(bin_name.split(&#39;_&#39;)[-1])-1
            t1 = int(bin_name.split(&#39;_&#39;)[-1])+1
    R={}
    for k in dict_id_reached_by_edgelabel:
        v = dict_id_reached_by_edgelabel[k]
        R[k]=np.median(
            np.array([self.deQuantizer(
                str(x).strip(),
                biome_prefix,
                timestamp_list=timestamp_list,
                time_start=t0,
                time_end=t1) for x in v]))
    return R</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.get_lowlevel"><code class="name flex">
<span>def <span class="ident">get_lowlevel</span></span>(<span>self, source, target, time_end=None, time_start=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Low level evaluation call to estimate local marginal regulation
<span><span class="MathJax_Preview"> \alpha </span><script type="math/tex"> \alpha </script></span></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code></dt>
<dd>source</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>target</dd>
<dt><strong><code>time_end</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>end time (Default value = None)</dd>
<dt><strong><code>time_start</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>start time (Default value = None)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_lowlevel(self,
        source,
        target,
        time_end=None,
        time_start=None):
    &#34;&#34;&#34;Low level evaluation call to estimate local marginal regulation  \( \\alpha \)

    Args:
      source (str): source
      target (str): target
      time_end (int, optional): end time (Default value = None)
      time_start (int, optional): start time (Default value = None)

    Returns:

    &#34;&#34;&#34;
    self.TGT = target
    self.SRC = source

    decision_trees = self.createTargetList(
        source,
        target,
        time_end,
        time_start)
    grad_=[]
    # can we do this in parallel
    for tree in decision_trees:
        self.TGT = os.path.basename(tree).replace(&#39;.dot&#39;,&#39;&#39;)
        gv = pgv.AGraph(tree,
                        strict=False,
                        directed=True)

        self.decision_tree = nx.DiGraph(gv)
        self.time_start = time_start
        self.time_end = time_end

        self.tree_labels = nx.get_node_attributes(
            self.decision_tree,&#39;label&#39;)
        self.tree_edgelabels = nx.get_edge_attributes(
            self.decision_tree,&#34;label&#34;)

        nodes_with_src=[]
        for (k,v) in self.tree_labels.items():
            if self.SRC in v:
                if self.src_time_constraint(v,self.TGT):
                    nodes_with_src=nodes_with_src+[k]

        if len(nodes_with_src)==0:
            continue

        RES=pd.concat([self.getHypothesisSlice(i).transpose()
                       for i in nodes_with_src])

        grad,pvalue=self.getAlpha(RES)
        #RES.to_csv(&#39;tmp.csv&#39;)
        #if RES.index.size &gt; 2:
        #    quit()

        #grad=stats.linregress(
        #    RES.x_.values,
        #    RES.muy.values).slope

        if np.isnan(grad):
            warnings.warn(
                &#34;Nan encountered in causal inferrence&#34;)
            grad=np.median(
                RES.y.values)/np.median(
                    RES.x.values)

        ns_ = re.split(r&#39;(.*)_(\d+)&#39;, self.TGT)
        self.hypotheses = self.hypotheses.append(
            {&#39;src&#39;:self.SRC,
             &#39;tgt&#39;:&#39;&#39;.join(ns_[:-2]),
             &#39;time_tgt&#39;:float(ns_[-2]),
             &#39;lomar&#39;:float(grad),
             &#39;pvalue&#39;:pvalue},
            ignore_index = True)
    return</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.get_vector_from_dict"><code class="name flex">
<span>def <span class="ident">get_vector_from_dict</span></span>(<span>self, str_alph_val)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate a probability distribution from string representation of
alphabet : value read from decision tree models</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_vector_from_dict(self,str_alph_val):
    &#34;&#34;&#34;Calculate a probability distribution from string representation of
       alphabet : value read from decision tree models

    &#34;&#34;&#34;
    vec_alph_val=str_alph_val.split()

    dict_label_float={}

    for x in vec_alph_val:
        y=x.split(&#39;:&#39;)
        dict_label_float[y[0]]=float(y[1])

    prob_dist = np.zeros(len(self.LABELS.keys()))
    for i in dict_label_float:
        prob_dist[self.LABELS[i]] = dict_label_float[i]


    return prob_dist/prob_dist.sum()</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.leaf_output_on_subgraph"><code class="name flex">
<span>def <span class="ident">leaf_output_on_subgraph</span></span>(<span>self, nodeset)</span>
</code></dt>
<dd>
<div class="desc"><p>Find the mean and sample standard deviation of output
in leafnodes reachable from nodeset, along with fraction of samples
captures by this subgraph</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nodeset</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>1D array of nodeids</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>tuple(float,float), float: mean, sample standard deviation and sample fraction</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def leaf_output_on_subgraph(self,nodeset):
    &#34;&#34;&#34;Find the mean and sample standard deviation of output
       in leafnodes reachable from nodeset, along with fraction of samples
       captures by this subgraph

    Args:
      nodeset (numpy.ndarray): 1D array of nodeids

    Returns:
      tuple(float,float), float: mean, sample standard deviation and sample fraction

    &#34;&#34;&#34;

    ## cLeaf is the set of leaf nodes reachable from nodeset
    # oLabels is the output labels for target and
    # is a dict mapping leafnode id to output label letter
    #
    # frac and prob are sample fraction and probability of
    # output label in leaf node, parsed from dotfile
    #
    # SUM is the total sample fraction captured by nodeset
    cLeaf=[x for x in nodeset
           if self.decision_tree.out_degree(x)==0
           and self.decision_tree.in_degree(x)==1]
    oLabels={k:str(v.split(&#39;\n&#39;)[0])
             for (k,v) in self.tree_labels.items()
             if k in cLeaf}

    frac={k:float(v.split(&#39;\n&#39;)[2].replace(&#39;Frac:&#39;,&#39;&#39;))
          for (k,v) in self.tree_labels.items()
          if k in cLeaf}
    if not self.detailed_labels:
        prob={k:float(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
              for (k,v) in self.tree_labels.items()
              if k in cLeaf}

        ## Get a kernel based distribution here.
        # self.alphabet=[&#39;A&#39;,...,&#39;E&#39;]
        # prob is regularize_distributioned to get a dict {nodeid: [p1,..,pm]}
        prob__={k:self.regularize_distribution(prob[k],oLabels[k])
                for k in prob}
        prob=prob__
    else:
        prob={k:self.get_vector_from_dict(v.split(&#39;\n&#39;)[1].replace(&#39;Prob:&#39;,&#39;&#39;))
              for (k,v) in self.tree_labels.items()
              if k in cLeaf}

    SUM=np.array(frac.values()).sum()

    ## mean and sample estimate of standard deviation
    mu_X,sigma_X=self.getNumeric_at_leaf(prob,frac)
    return (mu_X,sigma_X),SUM</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.regularize_distribution"><code class="name flex">
<span>def <span class="ident">regularize_distribution</span></span>(<span>self, prob, l, e=0.005)</span>
</code></dt>
<dd>
<div class="desc"><p>Regularize probability distribution
using exponential decay to map non-detailed output of a
single maximum likelihood label to a probability distribution.
Used when detailed output is not available.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prob</code></strong> :&ensp;<code>float</code></dt>
<dd>probability of single output label of type str</dd>
<dt><strong><code>e</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>small value to regularize return probability of 1.0 (Default value = 0.005)</dd>
<dt><strong><code>l</code></strong> :&ensp;<code>str</code></dt>
<dd>output label</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>probability distribution</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regularize_distribution(self,prob,l,e=0.005):
    &#34;&#34;&#34;Regularize probability distribution
       using exponential decay to map non-detailed output of a
       single maximum likelihood label to a probability distribution.
       Used when detailed output is not available.

    Args:
      prob (float): probability of single output label of type str
      e (float, optional): small value to regularize return probability of 1.0 (Default value = 0.005)
      l (str): output label

    Returns:
      numpy.ndarray: probability distribution

    &#34;&#34;&#34;
    labels=np.array(list(self.LABELS.keys()))
    yy=np.ones(len(labels))*((1-prob-e)/(len(labels)-1))
    yy[np.where(labels==l)[0][0]]=prob-e
    dy=pd.DataFrame(yy).ewm(alpha=.8).mean()
    dy=dy/dy.sum()
    return dy.values</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.src_time_constraint"><code class="name flex">
<span>def <span class="ident">src_time_constraint</span></span>(<span>self, source, target)</span>
</code></dt>
<dd>
<div class="desc"><p>Constrain which time points for source can be
considered as inputs when computing source to target influences.
If causal_constraint is None, there are no restrictions.
Otherwise, only sources that are at least causal_constraint
units of time prior to the target may be considered. Default is 0.
Negative values are possible.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code></dt>
<dd>full source name in biome_timestamp format</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>str</code></dt>
<dd>full target name in biome_timestamp format</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Truth value of src acceptance</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def src_time_constraint(self,source,target):
    &#34;&#34;&#34;Constrain which time points for source can be
       considered as inputs when computing source to target influences.
       If causal_constraint is None, there are no restrictions.
       Otherwise, only sources that are at least causal_constraint
       units of time prior to the target may be considered. Default is 0.
       Negative values are possible.

    Args:
      source (str): full source name in biome_timestamp format
      target (str): full target name in biome_timestamp format

    Returns:
      bool: Truth value of src acceptance

    &#34;&#34;&#34;
    _, source_biome_full, source_timestamp, _ \
        = re.split(r&#39;(.*)_(\d+)&#39;, source)
    _, target_biome_full, target_timestamp, _ \
        = re.split(r&#39;(.*)_(\d+)&#39;, target)

    if self.causal_constraint is not None:
        if (float(source_timestamp)
        + self.causal_constraint
            &lt;= float(target_timestamp)):
            return True
        return False
    return True</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.to_csv"><code class="name flex">
<span>def <span class="ident">to_csv</span></span>(<span>self, *args, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Output csv of hypotheses inferred. Arguments are passed to pandas.DataFrame.to_csv()</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*args</code></strong></dt>
<dd>optional arguments to <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html">pandas.to_csv()</a></dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>optional keywords to <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html">pandas.to_csv()</a></dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_csv(self, *args, **kwargs):
    &#34;&#34;&#34;Output csv of hypotheses inferred. Arguments are passed to pandas.DataFrame.to_csv()

    Args:
      *args: optional arguments to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)
      **kwargs: optional keywords to [pandas.to_csv()]( https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)

    Returns:

    &#34;&#34;&#34;
    self.hypotheses.to_csv(*args, **kwargs)</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.to_dot"><code class="name flex">
<span>def <span class="ident">to_dot</span></span>(<span>self, filename='tmp.dot', hypotheses=None, square_mat=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Output dot file of hypotheses inferred.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>filename of dot outpt (Default value = 'tmp.dot')</dd>
<dt>hypotheses (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">pandas.DataFrame</a>, optional): If provided use this instead of self.hypotheses (Default value = None)</dt>
<dt><strong><code>square_mat</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If True resturn a heatmap matrix as filename+'sq.csv' (Default value = False)</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dot(self,filename=&#39;tmp.dot&#39;,
           hypotheses=None,
           square_mat=False):
    &#34;&#34;&#34;Output dot file of hypotheses inferred.

    Args:
      filename (str, optional): filename of dot outpt (Default value = &#39;tmp.dot&#39;)
      hypotheses ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), optional): If provided use this instead of self.hypotheses (Default value = None)
      square_mat (bool, optional): If True resturn a heatmap matrix as filename+&#39;sq.csv&#39; (Default value = False)

    Returns:

    &#34;&#34;&#34;

    if hypotheses is None:
        df=self.hypotheses.copy()
    else:
        df=hypotheses

    df=df.groupby([&#39;src&#39;,&#39;tgt&#39;]).median().reset_index()
    df=df.pivot(index=&#39;src&#39;,columns=&#39;tgt&#39;,values=&#39;lomar&#39;)
    df=df.fillna(0)

    index = df.index.union(df.columns)
    df = df.reindex(index=index, columns=index, fill_value=0)

    df=df.sort_index()
    if square_mat:
        df.to_csv(filename.replace(&#39;.dot&#39;,&#39;sq.csv&#39;))

    G = nx.from_pandas_adjacency(df,create_using=nx.DiGraph())

    from networkx.drawing.nx_agraph import write_dot
    write_dot(G,filename)

    return</code></pre>
</details>
</dd>
<dt id="qbiome.hypothesis.Hypothesis.trim_hypothesis"><code class="name flex">
<span>def <span class="ident">trim_hypothesis</span></span>(<span>self, alternate_hypothesis_dataframe)</span>
</code></dt>
<dd>
<div class="desc"><p>Compate current hypothesis dataframe with alternate_hypothesis_dataframe</p>
<h2 id="args">Args</h2>
<p>alternate_hypothesis_dataframe (<a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">pandas.DataFrame</a>): alternate dataframe</p>
<h2 id="returns">Returns</h2>
<p><a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html">pandas.DataFrame</a>: manipulated dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def trim_hypothesis(self,alternate_hypothesis_dataframe):
    &#34;&#34;&#34;Compate current hypothesis dataframe with alternate_hypothesis_dataframe

    Args:
      alternate_hypothesis_dataframe ([pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)): alternate dataframe

    Returns:
      [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html): manipulated dataframe

    &#34;&#34;&#34;
    df=self.hypotheses.copy()

    #df.set_index([&#39;src&#39;,&#39;tgt&#39;]).merge


    return df</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<img src="logozed_nowhite.png" alt="drawing" style="width:400px;"/>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qbiome" href="index.html">qbiome</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="qbiome.hypothesis.Hypothesis" href="#qbiome.hypothesis.Hypothesis">Hypothesis</a></code></h4>
<ul class="">
<li><code><a title="qbiome.hypothesis.Hypothesis.createTargetList" href="#qbiome.hypothesis.Hypothesis.createTargetList">createTargetList</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.deQuantize_lowlevel" href="#qbiome.hypothesis.Hypothesis.deQuantize_lowlevel">deQuantize_lowlevel</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.deQuantizer" href="#qbiome.hypothesis.Hypothesis.deQuantizer">deQuantizer</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.get" href="#qbiome.hypothesis.Hypothesis.get">get</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.getAlpha" href="#qbiome.hypothesis.Hypothesis.getAlpha">getAlpha</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.getHypothesisSlice" href="#qbiome.hypothesis.Hypothesis.getHypothesisSlice">getHypothesisSlice</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.getNumeric_at_leaf" href="#qbiome.hypothesis.Hypothesis.getNumeric_at_leaf">getNumeric_at_leaf</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.getNumeric_internal" href="#qbiome.hypothesis.Hypothesis.getNumeric_internal">getNumeric_internal</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.get_lowlevel" href="#qbiome.hypothesis.Hypothesis.get_lowlevel">get_lowlevel</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.get_vector_from_dict" href="#qbiome.hypothesis.Hypothesis.get_vector_from_dict">get_vector_from_dict</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.leaf_output_on_subgraph" href="#qbiome.hypothesis.Hypothesis.leaf_output_on_subgraph">leaf_output_on_subgraph</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.regularize_distribution" href="#qbiome.hypothesis.Hypothesis.regularize_distribution">regularize_distribution</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.src_time_constraint" href="#qbiome.hypothesis.Hypothesis.src_time_constraint">src_time_constraint</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.to_csv" href="#qbiome.hypothesis.Hypothesis.to_csv">to_csv</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.to_dot" href="#qbiome.hypothesis.Hypothesis.to_dot">to_dot</a></code></li>
<li><code><a title="qbiome.hypothesis.Hypothesis.trim_hypothesis" href="#qbiome.hypothesis.Hypothesis.trim_hypothesis">trim_hypothesis</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
Author: Lynn Zheng, Jin Li and Ishanu Chattopadhyay <a href="https://zed.uchicago.edu"> Zero Knowledge Discovery, University of Chicago</a>. Email: ishanu@uchicago.edu
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>